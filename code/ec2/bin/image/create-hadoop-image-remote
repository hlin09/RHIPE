#!/bin/sh

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Create a Hadoop AMI. Runs on the EC2 instance.

# Import variables
echo "LOGGED IN, BEGIN INSTALL"

. ./hadoop-ec2-env.sh

# Remove environment script since it contains sensitive information
rm -f ./hadoop-ec2-env.sh

# Install Java
echo "Download java"
wget -nv -O /usr/local/java.bin "$JAVA_BINARY_URL"
echo "Installing java binary."
cd /usr/local
sh java.bin
rm -f java.bin

# Install tools
echo "$CRAN_REPO" >>/etc/apt/sources.list
apt-get update
echo "Installing packages."
apt-get install -y rsync lynx screen  apache2 php5-cli php5-dev  php5-common ant ant-doc ant-optional git liblzo2-2 liblzo2-dev zlib1g-dev  zlibc gzip bzip2   python  python-numeric 

apt-get install -y r-base r-base-dev  python-rpy ess r-recommended r-cran-rodbc r-cran-rmysql
apt-get install -y ganglia-monitor libganglia1 libganglia1-dev   
# ganglia-gmetad ganglia-gmond ganglia-web

#install protobuf
export PROTOVER=2.2.0a
wget -nv http://protobuf.googlecode.com/files/protobuf-$PROTOVER.tar.bz2 -P /tmp
cd /tmp
tar jxvf protobuf-$PROTOVER.tar.bz2
cd protobuf-$PROTOVER
./configure
make install

export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH
export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}
export PATH=/usr/local/jdk${JAVA_VERSION}/bin:$PATH

# Install Hadoop
echo "Installing Hadoop $HADOOP_VERSION."
cd /usr/local
# wget -nv http://archive.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
wget http://ml.stat.purdue.edu/rhipeimage/hadoop-$HADOOP_VERSION.tar.gz
# [ ! -f hadoop-$HADOOP_VERSION.tar.gz ] && wget -nv #http://www.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
tar xzf hadoop-$HADOOP_VERSION.tar.gz
rm -f hadoop-$HADOOP_VERSION.tar.gz

# Configure Hadoop
sed -i -e "s|# export JAVA_HOME=.*|export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}|" \
       -e 's|# export HADOOP_LOG_DIR=.*|export HADOOP_LOG_DIR=/mnt/hadoop/logs|' \
       -e 's|# export HADOOP_SLAVE_SLEEP=.*|export HADOOP_SLAVE_SLEEP=1|' \
       -e 's|# export HADOOP_OPTS=.*|export HADOOP_OPTS=-server|' \
      /usr/local/hadoop-$HADOOP_VERSION/conf/hadoop-env.sh

# Run user data as script on instance startup
chmod +x /etc/init.d/ec2-run-user-data
echo "/etc/init.d/ec2-run-user-data" >> /etc/rc.d/rc.local

# Setup root user bash environment
echo 'export PATH=/usr/local/jdk$JAVA_VERSION/bin:$PATH' >>  /root/.bash_profile
echo "export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}" >> /root/.bash_profile
echo "export HADOOP=/usr/local/hadoop-${HADOOP_VERSION}" >> /root/.bash_profile
echo "export HADOOP_HOME=/usr/local/hadoop-${HADOOP_VERSION}" >> /root/.bash_profile
echo 'export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH' >> /root/.bash_profile
echo 'export HADOOP_BIN=$HADOOP_HOME/bin' >> /root/.bash_profile
echo 'export HADOOP_LIB=$HADOOP_HOME/lib'>> /root/.bash_profile
echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/conf' >>  /root/.bash_profile
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native' >> /root/.bash_profile
echo "export CLASSPATH=$HADOOP_CONF_DIR:$HADOOP:$HADOOP/lib:" >> /root/.bash_profile
echo "export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig"  >> /root/.bash_profile

echo "Configuring R for Java"
R CMD javareconf

## Install hadoop-gpl-compression
## Requires hadoop 0.20 +
# cd /usr/local
# svn checkout http://hadoop-gpl-compression.googlecode.com/svn/trunk/ hadoop-gpl-compression-read-only
# cd hadoop-gpl-compression-read-only
# rm lib/*jar
# cp /usr/local/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}*jar lib/
# ant compile-native

# cp build/*jar /usr/local/hadoop-${HADOOP_VERSION}/lib/native/


# ## Build native zip libsfor hadoop
# cd /usr/local/hadoop-${HADOOP_VERSION}
# ant -Dcompile.native=true compile-core-native
# cp build/native/*/lib/* /usr/local/hadoop-${HADOOP_VERSION}/lib/native/

## Install rJava
wget -q http://www.rforge.net/rJava/snapshot/rJava_0.8-2.tar.gz -P /tmp
R CMD INSTALL /tmp/rJava_0.8-2.tar.gz
# rm -rf /tmp/Rserve_0.6-0.tar.gz


# Configure networking.
# Delete SSH authorized_keys since it includes the key it was launched with. (Note that it is re-populated when an instance starts.)
rm -f /root/.ssh/authorized_keys
# Ensure logging in to new hosts is seamless.
echo '    StrictHostKeyChecking no' >> /etc/ssh/ssh_config

# Bundle and upload image
cd ~root
# Don't need to delete .bash_history since it isn't written until exit.
df -h
echo "EC2-BUNDLE"
ec2-bundle-vol -d /mnt -k /mnt/pk*.pem -c /mnt/cert*.pem -u $AWS_ACCOUNT_ID -s 3072 -p rhipe-hadoop-$HADOOP_VERSION-$ARCH -r $ARCH

echo "EC2-UPLOAD"
ec2-upload-bundle --retry  -b $S3_BUCKET -m /mnt/rhipe-hadoop-$HADOOP_VERSION-$ARCH.manifest.xml -a $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY

# End
echo Done
