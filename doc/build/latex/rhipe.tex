% Generated by Sphinx.
\documentclass[letterpaper,10pt,english]{manual}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}


\title{rhipe Documentation}
\date{September 01, 2009}
\release{0.5}
\author{Saptarshi Guha}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex
\makemodindex
\newcommand\PYGZat{@}
\newcommand\PYGZlb{[}
\newcommand\PYGZrb{]}
\newcommand\PYGaz[1]{\textcolor[rgb]{0.00,0.63,0.00}{#1}}
\newcommand\PYGax[1]{\textcolor[rgb]{0.84,0.33,0.22}{\textbf{#1}}}
\newcommand\PYGay[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGar[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGas[1]{\textcolor[rgb]{0.25,0.44,0.63}{\textit{#1}}}
\newcommand\PYGap[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaq[1]{\textcolor[rgb]{0.38,0.68,0.84}{#1}}
\newcommand\PYGav[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaw[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGat[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGau[1]{\textcolor[rgb]{0.32,0.47,0.09}{#1}}
\newcommand\PYGaj[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGak[1]{\textcolor[rgb]{0.14,0.33,0.53}{#1}}
\newcommand\PYGah[1]{\textcolor[rgb]{0.00,0.13,0.44}{\textbf{#1}}}
\newcommand\PYGai[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGan[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGao[1]{\textcolor[rgb]{0.25,0.44,0.63}{\textbf{#1}}}
\newcommand\PYGal[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGam[1]{\textbf{#1}}
\newcommand\PYGab[1]{\textit{#1}}
\newcommand\PYGac[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaa[1]{\textcolor[rgb]{0.19,0.19,0.19}{#1}}
\newcommand\PYGaf[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGag[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGad[1]{\textcolor[rgb]{0.00,0.25,0.82}{#1}}
\newcommand\PYGae[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaZ[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGbf[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaX[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaY[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGbc[1]{\textcolor[rgb]{0.78,0.36,0.04}{#1}}
\newcommand\PYGbb[1]{\textcolor[rgb]{0.00,0.00,0.50}{\textbf{#1}}}
\newcommand\PYGba[1]{\textcolor[rgb]{0.02,0.16,0.45}{\textbf{#1}}}
\newcommand\PYGaR[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaS[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaP[1]{\textcolor[rgb]{0.05,0.52,0.71}{\textbf{#1}}}
\newcommand\PYGaQ[1]{\textcolor[rgb]{0.78,0.36,0.04}{\textbf{#1}}}
\newcommand\PYGaV[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGaW[1]{\textcolor[rgb]{0.05,0.52,0.71}{\textbf{#1}}}
\newcommand\PYGaT[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGaU[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaJ[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand\PYGaK[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaH[1]{\textcolor[rgb]{0.50,0.00,0.50}{\textbf{#1}}}
\newcommand\PYGaI[1]{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{#1}}
\newcommand\PYGaN[1]{\textcolor[rgb]{0.73,0.73,0.73}{#1}}
\newcommand\PYGaO[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaL[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand\PYGaM[1]{\colorbox[rgb]{1.00,0.94,0.94}{\textcolor[rgb]{0.25,0.50,0.56}{#1}}}
\newcommand\PYGaB[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaC[1]{\textcolor[rgb]{0.33,0.33,0.33}{\textbf{#1}}}
\newcommand\PYGaA[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaF[1]{\textcolor[rgb]{0.63,0.00,0.00}{#1}}
\newcommand\PYGaG[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand\PYGaD[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaE[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGbg[1]{\textcolor[rgb]{0.44,0.63,0.82}{\textit{#1}}}
\newcommand\PYGbe[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand\PYGbd[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGbh[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\begin{document}

\maketitle
\tableofcontents



Mainpage

\resetcurrentobjects
\hypertarget{--doc-installation}{}

\chapter{Setting up RHIPE}


\section{Requirements}

RHIPE requires Java (JDK 1.6, checked only with Sun's Java), and R.
Currently, RHIPE only works on Linux, though one day it might also
work on Windows too.


\section{Installation}
\begin{enumerate}
\item {} 
Install Java and R

\item {} 
Install rJava, Simon Urbanek's R package to allow R to call Java libraries.

\item {} 
Download the current version of RHIPE, untar and run

\end{enumerate}

\begin{Verbatim}[commandchars=@\[\]]
make R
\end{Verbatim}

This will build and install the R RHIPE package. There is a JAR file
present in the \code{lib/} folder, but should you wish to build the RHIPE
JAR file, set the environment variables \code{HADOOP} should point to the
Hadoop distribution folder. To build the JAR file, run

\begin{Verbatim}[commandchars=@\[\]]
make java
\end{Verbatim}

To build the docs, install \href{http://sphinx.pocoo.org/}{sphynx}
\begin{enumerate}
\item {} 
Given that Hadoop is running, typing

\end{enumerate}

\begin{Verbatim}[commandchars=@\[\]]
@PYGZlb[]bash@PYGZrb[] R
@PYGbe[@textgreater[]]@PYGbe[@textgreater[]] library(rhipe)
\end{Verbatim}

should get you started.

\resetcurrentobjects
\hypertarget{--doc-rhlapply}{}

\chapter{The \texttt{rhlapply} Command}


\section{Introduction}

\code{rhapply} applies a user defined function to the elements of a given
R list or the function can be run over the set of numbers from 1 to
n. In the former case the list is written to a sequence file, the
default is to write \emph{one sequence file} per item, which can be grossly
inefficient. This can be controlled by setting \code{mapred.max.tasks} in
the \code{hadoop.options} paramater of \code{rhlapply}.

Running a hundreds of thousadands of seperate trials
can be terribly inefficient, instead consider grouping them, i.e set
\code{mapred.max.tasks} to a value much smaller than the length of the
list.


\section{Return Value}

\code{rhlapply} returns a list, the names of which is equal to the names
of the input list (if given).


\section{Function Usage}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rhlapply @PYGbe[@textless[]-] @PYGap[function]( list.object,
                     func,
                     configure@PYGbe[=]expression(),
                     output.folder@PYGbe[=]'',
                     shared.files@PYGbe[=]c(),
                     hadoop.mapreduce@PYGbe[=]list(),
                     verbose@PYGbe[=]T,
                     takeAll@PYGbe[=]T)
\end{Verbatim}

Description of the options follows.
\begin{description}
\item[\code{list.object}]
Either an R list or a scalar number, N. In the former case, the
user function gets the value. In the latter, the function gets
the iteration number (from 1 to N)

\item[\code{func}]
A user supplied R function, which can return any R object.

\item[\code{configure}]
An expression which can be used to setup the environment. Run
once for every \emph{task}. So if the user has 100K tasks, it is run
100K times.

\item[\code{output.folder}]
Save the results in an output folder, not required, but this
output folder can be used \code{rhmr} or to \code{rhlapply}

\item[\code{shared.files}]
A character vector of files located on the HDFS. The files are
copied to a working folder on the cluster machines, thus
speeding up reads. To load the filename, use the trailing part
of the filename, e.g

\end{description}

\begin{Verbatim}[commandchars=@\[\]]
rhlapply(...,shared.files@PYGbe[=]c('@PYGbe[/]tmp@PYGbe[/]file1.Rdata','@PYGbe[/]user@PYGbe[/]me@PYGbe[/]obj.Rdata'))
\end{Verbatim}

Then e.g. in R,

\begin{Verbatim}[commandchars=@\[\]]
configure @PYGbe[@textless[]-] expression({
        load('obj.Rdata')
        load('file1.Rdata')
        })
\end{Verbatim}

Note, the files are copied to R's working directory.
\begin{description}
\item[\code{hadoop.mapreduce}]
A list, keys are hadoop options. Use this to set all of the
Hadoop options.

\item[\code{verbose}]
If True, the progress of the job appears in the R terminal,
else a web url is provided for the user to inspect. Also in
the latter, control returns immediately to the user

\item[\code{takeAll}]
Get back all the results from the temporary output file.

\end{description}

\resetcurrentobjects
\hypertarget{--doc-rhmr}{}

\chapter{The \texttt{rhmr} Command}


\section{Introduction}

The \code{rhmr} command runs a general mapreduce program using user supplied map
and reduce commands.


\section{Return Value}

In general a set of files on the Hadoop Distributed File System. It can be of
Text Format or a Sequence file format. In case of the latter, the key and values
can be any R data structure.


\section{Function}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rhmr @PYGbe[@textless[]-] @PYGap[function](map,reduce,input.folder,configure@PYGbe[=]list(map@PYGbe[=]expression(),reduce@PYGbe[=]expression()),
        close@PYGbe[=]list(map@PYGbe[=]expression(),reduce@PYGbe[=]expression())
         output.folder@PYGbe[=]'',combiner@PYGbe[=]F,step@PYGbe[=]F,
         shared.files@PYGbe[=]c(),inputformat@PYGbe[=]@PYGaB["]@PYGaB[TextInputFormat"],
         outputformat@PYGbe[=]@PYGaB["]@PYGaB[TextOutputFormat"],
         hadoop.mapreduce@PYGbe[=]list(),verbose@PYGbe[=]T,libjars@PYGbe[=]c())
\end{Verbatim}
\begin{description}
\item[input.folder]
A folder on the DFS containing the files to process. Can be a vector.

\item[output.folder]
A folder on the DFS where output will go to.

\item[inputformat]
Either of \code{TextInputFormat} or \code{SequenceFileInputFormat}. Use the former
for text files and the latter for sequence files created from within R or as
outputs from RHIPE(e.g \code{rhlapply} or \code{rhmr}).

\end{description}

\begin{notice}{note}{Note:}
one can't use any sequence file, they must have been created via a RHIPE function. Custom Input formats are also possible. Download the source and look at \code{code/java/RXTextInputFormat.java}
\end{notice}
\begin{description}
\item[outputformat]
Either of \code{TextOutputFormat} or \code{SequenceFileOutputFormat}. In case of the former, the return value from the mapper or reducer is converted to character and written to disk. The following code is used to convert to character.

\end{description}

\begin{Verbatim}[commandchars=@\[\]]
paste(key,sep@PYGbe[=]'',collapse@PYGbe[=]field@_separator)
\end{Verbatim}

Custom output formats are also possible. Download the source and look at \code{code/java/RXTextOutputFormat.java}

If custom formats implement their own writables, it must subclass RXWritable or use one of the writables presents in RHIPE
\begin{description}
\item[shared.files]
same as in \code{rhlapply}, see that for documentation.

\item[verbose]
If T, the job progress is displayed. If false, then the job URL is displayed.

\end{description}

At any time in the configure, close, map or reduce function/expression, the variable \code{mapred.task.is.map} will be equal to TRUE if it is map task,FALSE otherwise (both strings) Also, \code{mapred.iswhat} is ``mapper'', ``reducer'', ``combiner'' in their respective environments.
\begin{description}
\item[configure]
A list with either one element (an expression) or two attributes ``map'' and ``reduce'' both of which must be expressions. These expressions are called in their respective environments, i.e the map expression is called during the map configure and similarly for the reduce expression. The reduce expression is called for the combiner configure method.

If only one list element, the expression is used for both the map and reduce

\item[close]
Same as configure .

\item[map]
a function that takes two values key and value. Should return a list of
lists. Each list entry must contain two elements, the first one is the key
and second the value (they do not have to be named), e.g.

\end{description}

\begin{Verbatim}[commandchars=@\[\]]
ret @PYGbe[@textless[]-] list()
ret@PYGZlb[]@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGZrb[] @PYGbe[@textless[]-]  list(key@PYGbe[=]c(@PYGaS[1],@PYGaS[2]), value@PYGbe[=]c('x','b'))
@PYGap[return](ret)
\end{Verbatim}

If any of key/value are missing the output is not collected, e.g. return NULL
to skip this record. If the input format is a \code{TextInputFormat}, the input
value is the entire line and the input key is probably useless to the user( it is a number indicating bytes into the file). If the input format is \code{SequenceFileInputFormat}, the key and value are taken from the sequence file.
\begin{description}
\item[reduce]
Not needed if \code{mapred.reduce.tasks} is 0. Takes a key and a list of
values( all values emitted from the maps that share the same map output key
). If \code{step} is True, then not a list. Must return a list of lists each
element of which must have two elements key and value. This collects all the
values and sends them to function. If NULL is returned nothing is collected
by the Hadoop collector

\item[step]
If step is TRUE, then the reduce function is called every batch of values
(the size of the batch is user configurable) corresponding to a key
\begin{itemize}
\item {} 
The variable \code{red.status} is equal to 1 on the first call.

\item {} 
\code{red.status} is equal to 0 for every subsequent calls including the last value

\item {} 
The reducer function is called one last time with \code{red.status} equal to -1. The value is NULL.

Anything returned at any of these stages is written to disk. The
\code{close} function is called once every value for a given key has been
processed, but returning anything has no effect. To assign to the
global environment use the \code{\textless{}\textless{}-} operator. To bail out at any time
(i.e after the third value you do not want to process anymore) and
move onto the next key, return a list with the attribute \code{stop} and
set it to 1.

\end{itemize}

\item[combiner]
TRUE or FALSE, to use the reducer as a combiner. Using a combiner makes computation more efficient. If combiner is true, the reduce function will be called as a combiner (0 or more times, it may never be called during the combine stage even if combiner is T) .

The value of \code{mapred.task.is.map} is ``true'' or ``false'' FALSE (both strings) if the combiner is being executed as part of the map stage or reduce stage respectively.

Whether knowledge of this is useful or not is something I'm not sure
of. However, if combiner is TRUE , keep in mind,your reduce function must be
able to handle inputs sent from the map or inputs sent from the reduce
function(itself).

\item[libjars]
If specifying a custom input/output format, the user might need to specify
jar files here.

\item[hadoop.mapreduce]
set RHIPE and Hadoop options via this.

\end{description}


\section{\texttt{hadoop.mapreduce} Options}

All the options that can be set for Hadoop can be set via
\code{hadoop.mapreduce}. Here is a list of some options RHIPE uses
\begin{description}
\item[rhipejob.rport]
The port on which Rserve runs.

\emph{default:} 8888

\item[rhipejob.outfmt.is.text]
1 if the output format is textual.

\emph{default:} 1

\item[rhipejob.textoutput.fieldsep]
The output field seperator.

\emph{default:} `'

\item[rhipejob.textinput.comment]
If the input is textual and a line begins with this, it is skipped.

\emph{default:} `\#'

\item[rhipejob.combinerspill]
This number of items are collected by the combiner before being sent to Rserve.

\emph{default:} 100,000

\item[rhipejob.tom]
Number of map key,values collated before being sent to the mapper (on Rserve).

\emph{default:} 200,000

\item[rhipejob.frommap]
Number of map output key,values to bring from Rserve in one go.

\emph{default:} 200,000

\item[rhipejob.tor.batch]
Number of reduce values to be sent to Reducer (batching).

\emph{default:} 200,000

\item[rhipejob.copy.to.dfs]
Copy side effect files from local to DFS?

\emph{default:} 1

\item[rhipejob.inputformat.keyclass]
Provide the full Java URL to the keyclass e.g \code{org.saptarshiguha.rhipe.hadoop.RXWritableText}, when using a Custom InputFormat implement RXWritable and implement the methods.

\emph{default:} The default is chosen depending on TextInputFormat or SequenceFileInputFormat.

\item[rhipejob.inputformat.valueclass]
Provide the full Java URL to the valueclass e.g \code{org.saptarshiguha.rhipe.hadoop.RXWritableText} when using a Custom InputFormat implement RXWritable and implement the methods.

\emph{default:}      The default is chosen depending on TextInputFormat or SequenceFileInputFormat.

\item[rhipejob.input.format.class]
Specify yours here.

\emph{default:}      As above, the default is either \code{org.saptarshiguha.rhipe.hadoop.RXTextInputFormat} or \code{org.apache.hadoop.mapred.SequenceFileInputFormat}

\item[rhipejob.outputformat.keyclass]
Provide the full Java URL to the value e.g \code{org.saptarshiguha.rhipe.hadoop.RXWritableText} , also the valueclass must implement RXWritable.

\emph{default:} The default is chosen depending on TextInputFormat or SequenceFileInputFormat

\item[rhipejob.outputformat.valueclass]
Provide the full Java URL to the value e.g \code{org.saptarshiguha.rhipe.hadoop.RXWritableText} , also the valueclass must implement RXWritable.

\emph{default:} The default is chosen depending on TextInputFormat or SequenceFileInputFormat

\item[rhipejob.output.format.class]
Specify yours here, provide libjars if required.

\emph{default:} As above, the default is either \code{org.saptarshiguha.rhipe.hadoop.RXTextOutputFormat} or \code{org.apache.hadoop.mapred.SequenceFileInputFormat}

\end{description}


\section{Custom Formats or Writables}

You can specify your own Input/Output Formats and Writables. See the source in
\code{code/java/} for a \code{RXTextInputFormat} which implements
\code{TextInputFormat}. The reason this was re implemented is because RHIPE
requires that all Writables (for the Key and Value) must implement RXWritable or
use one of the Writables in RHIPE e.g RXWritableLong, RXWritableText,
RXWritableDouble,RXWritableRAW


\section{Side Effect files}

Files written to \code{tmp/} (no leading slash !) e.g \code{pdf("tmp/x.pdf")} will be copied to the output folder.

\resetcurrentobjects
\hypertarget{--doc-rhmisc}{}

\chapter{Miscellaneous Commands}


\section{Introduction}

This is a list of supporting functions for reading, writing sequence files and
manipulating files on the Hadoop Distributed File System (HDFS).


\section{HDFS Related}


\subsection{rhget}

\begin{Verbatim}[commandchars=@\[\]]
rhget@PYGbe[@textless[]-] @PYGap[function](from,to,...)
\end{Verbatim}

Copies \code{from} from the DFS to \code{to} on the local filesystem. This uses the
system command and extra parameters are sent to the \code{system} command.


\subsection{rhput}

\begin{Verbatim}[commandchars=@\[\]]
rhput@PYGbe[@textless[]-] @PYGap[function](from,to,...)
\end{Verbatim}

The reverse of \code{rhget}.


\subsection{rhrm}

\begin{Verbatim}[commandchars=@\[\]]
rhrm @PYGbe[@textless[]-] @PYGap[function](w,gp@PYGbe[=]@PYGaD[NA],...)
\end{Verbatim}

Deletes the file \code{w} from the HDFS. Regular expressions can be given in
\code{gp}, if so \code{w} should be a folder. For example:

\begin{Verbatim}[commandchars=@\[\]]
rhrm(@PYGaB["]@PYGaB[/tmp"],gp@PYGbe[=]@PYGaB["]@PYGaB[partfile@$"])
\end{Verbatim}

deletes all files in \code{/tmp/} that end in \emph{partfile}


\subsection{rhls}

\begin{Verbatim}[commandchars=@\[\]]
rhls @PYGbe[@textless[]-] @PYGap[function](w@PYGbe[=]@PYGaB["]@PYGaB[/"])
\end{Verbatim}

Lists the files in the directory \code{w}.


\subsection{rhlsd}

\begin{Verbatim}[commandchars=@\[\]]
rhlsd @PYGbe[@textless[]-] @PYGap[function](fold,....)
\end{Verbatim}

Returns a \emph{data frame} of files in folder \code{fold}. Wildcards are allowed in
\code{fold}.


\section{Hadoop Job Related}

These are related to the killing of jobs.


\subsection{rhkill}

\begin{Verbatim}[commandchars=@\[\]]
rhkill @PYGbe[@textless[]-] @PYGap[function](w)
\end{Verbatim}

Kills the job with job id equal to \code{ww}. The job title can be either of
\emph{job2009051815100003} or \emph{2009051815100003} (i.e same as the former with the job
prefix removed).


\section{Saving R Datasets}


\subsection{rhsave.image, rhsave, rhload}

\begin{Verbatim}[commandchars=@\[\]]
rhsave.image @PYGbe[@textless[]-] @PYGap[function](file,...)
rhsave @PYGbe[@textless[]-] @PYGap[function](file,....)
rhload(file, envir@PYGbe[=]@PYGaD[NULL],...)
\end{Verbatim}

Similar to \code{save.image}, \code{save} and \code{load} except that it saves(or reads)
\code{file} to(or from) the HDFS.


\section{Creating Sequence FIles}

The following are useful for creating and reading sequence files created while
using RHIPE.


\subsection{rhnewSqfile}

\begin{Verbatim}[commandchars=@\[\]]
rhnewSqfile @PYGbe[@textless[]-] @PYGap[function](l,name,byrow@PYGbe[=]T,f@PYGbe[=]@PYGaD[NA],pct@PYGbe[=]@PYGaS[0.1])
\end{Verbatim}

Takes an object and writes its rows(columns if \code{byrow} is FALSE and \code{l} is a matrix/data frame) or its elements if \code{l} is a list or a scalar vector to a file on the DFS given by name. Uses default compression codec.

Displays progress output at \code{pct} intervals. If not NA, \code{rhnewSqfile} calls \code{f} on every element, so this can be used to pass a character vector corresponding to the objects to be written.

The keys correspond to row names(or column names) or names of the object.


\subsection{rhsqallKV}

\begin{Verbatim}[commandchars=@\[\]]
rhsqallKV @PYGbe[@textless[]-] @PYGap[function](ford,n,verbose@PYGbe[=]F,ignore@PYGbe[=]T,local@PYGbe[=]F,...)
\end{Verbatim}

Reads \emph{all} key(or a maximum of \code{n}),values from the sequence file
\code{ford}. Also, if \code{ford} is a folder of sequence files, one
can use wildcards as an appropiate value (e.g \code{tmp/p*})

If \code{verbose} is TRUE, the current file being read is displayed.

Returns a list of values with names corresponding to the keys.


\section{Low Level Functions}

The above two sequence files functions are written using the following functions


\subsection{rhsqreader}

\begin{Verbatim}[commandchars=@\[\]]
rhsqreader @PYGbe[@textless[]-] @PYGap[function](ford,local@PYGbe[=]F,...)
\end{Verbatim}

\code{ford} is the path to a sequence file or folder containing sequence
files.
\begin{quote}

Wildcards are allowed(if \code{local} is FALSE), so to process all part-xxx
\end{quote}

files in a folder use \code{rhsqreader("/x/p*")}.

To obtain wildcard functionality, use a pattern. For example,
\code{rhsqreader("/x",local=T,pattern="\textasciicircum{}p")}. is equivalent to the previous call,
assuming \code{/x} exists on the local filesystem.This function will not go into
subdirectories.
\begin{quote}

This function creates an object to read key/value pairs from the
\end{quote}

set of sequence files. After calling this, call \code{rhsqnextKVR} to retrieve
key-value pairs.


\subsection{rhsqnextKVR}

\begin{Verbatim}[commandchars=@\[\]]
rhsqnextKVR @PYGbe[@textless[]-] @PYGap[function](sqro)
\end{Verbatim}

Returns a list with two entries key and value. If none available, returns
NULL. \code{sqro} is the object returned by \code{rhsqreader}.


\subsection{rhsqnextpath}

\begin{Verbatim}[commandchars=@\[\]]
rhsqnextpath @PYGbe[@textless[]-] @PYGap[function](sqro)
\end{Verbatim}

When \code{rhsqreader} is used to read key-values from a folder of sequence files,
this function switches to the next file in the folder.

\begin{notice}{note}{Note:}
This is used internally by \code{rhsqreader} and \code{rhsqnextKVR}, the user does not need to use this.
\end{notice}


\subsection{rhsqclose}

\begin{Verbatim}[commandchars=@\[\]]
rhsqclose @PYGbe[@textless[]-] @PYGap[function](sqro)
\end{Verbatim}

Closes an sqreader object. \code{sqro} is a sqreader object. Used internally by
other functions.


\section{Example}


\subsection{Reading}

Two equivalent ways to read all key value pairs in folder of sequence files.

\textbf{Long Way}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rdr @PYGbe[@textless[]-] rhsqreader(@PYGaB["]@PYGaB[/test/one/p*"])
@PYGap[while](@PYGaD[TRUE]){
    value @PYGbe[@textless[]-]rhsqnextKVR(rdr)
    @PYGap[if](is.null(value)) {
        rdr @PYGbe[@textless[]-] rhsqnextpath(rdr)
      @PYGap[if](is.null(rdr)) @PYGap[break];
    }
    print(value@$key)
   print(value@$value)
}
\end{Verbatim}

\textbf{Short Way}

\begin{Verbatim}[commandchars=@\[\]]
info@PYGbe[@textless[]-]rhsqallKV(@PYGaB["]@PYGaB[/test/one/p*"],ignore@PYGbe[=]F)
\end{Verbatim}


\subsection{Writing}

\begin{Verbatim}[commandchars=@\[\]]
x @PYGbe[@textless[]-] matrix(runif(@PYGaS[1],@PYGaS[2],@PYGaS[3],@PYGaS[4],@PYGaS[5],@PYGaS[6]),ncol@PYGbe[=]@PYGaS[2])
rownames(x) @PYGbe[@textless[]-] c('a','b','c','d','e','f')
rhnewSqfile(x,byrow@PYGbe[=]T,name@PYGbe[=]@PYGaB["]@PYGaB[testfile"])
\end{Verbatim}

\resetcurrentobjects
\hypertarget{--doc-ec2}{}

\chapter{Using RHIPE on EC2}


\section{Introduction}

We have release two AMIs(32 and 64bit). Both are based on Fedora 8 and have
Hadoop 0.19.1,R 2.8 and RIPE (latest) installed. \href{http://s3sync.net/wiki}{s3sync} is also present.
\begin{description}
\item[32 bit]
ami-4b678122

\item[64 bit]
ami-9f7492f6

\end{description}

The following describes the usage of the EC2 scripts.


\section{Usage}
\begin{itemize}
\item {} 
Get an Amazon EC2 account and confirm the ability to start and instance from the command line (using ec2-tools).

\item {} 
Unzip the rhipe-ec2 distribution (see the downloads page)

\item {} 
OPTIONS

\end{itemize}

In \code{bin/hadoop-ec2-env.sh} template there are several options:
\begin{description}
\item[AWS\_ACCOUNT\_ID]
fill this from the Amazon Account Identifiers

\item[AWS\_ACCESS\_KEY\_ID]
same as above

\item[AWS\_SECRET\_ACCESS\_KEY]
same as above

\item[RSOPTS]
options to Rserve, default:

\end{description}

\begin{Verbatim}[commandchars=@\[\]]
-max-nsize@PYGbe[=]1G --max-ppsize@PYGbe[=]100000 --RS-port 8888
\end{Verbatim}
\begin{description}
\item[R\_USER\_FILE]
a URL to an R script. This file is executed on machine boot up. Useful to install R packages. Read \code{bin/hadoop-ec2-env.sh.template} for details.

\item[INSTANCE\_TYPE]
choose the Amazon machine instance type. For details, go to
\href{http://aws.amazon.com/ec2/instance-types/}{http://aws.amazon.com/ec2/instance-types/}

\end{description}
\begin{itemize}
\item {} 
Save the file as \code{bin/hadoop-ec2-env.sh}

\end{itemize}


\section{Some launch commands}
\begin{itemize}
\item {} 
launch

\end{itemize}

\begin{Verbatim}[commandchars=@\[\]]
bin/hadoop-ec2 launch-cluster clustername number-of-workers
\end{Verbatim}

Replace clustername with the name of the cluster and number-of-workers with the number of workers. Use Elasticfox to check all the instances are running, this can some time.
\begin{itemize}
\item {} 
login

\end{itemize}

\begin{Verbatim}[commandchars=@\[\]]
bin/hadoop-ec2 login clustername
\end{Verbatim}
\begin{itemize}
\item {} 
terminate

\end{itemize}

\begin{Verbatim}[commandchars=@\[\]]
bin/hadoop-ec2 terminate-cluster clustername
\end{Verbatim}
\begin{itemize}
\item {} 
You can check the status of jobs at masterip:50030 in your web browser.

\end{itemize}


\section{Useful tools}
\begin{description}
\item[\href{http://www.s3fox.net/}{s3fox}]
A S3 file browser that works within Firefox.

\item[\href{http://sourceforge.net/projects/elasticfox/}{Elasticfox}]
EC2 management tools, a Firefox add-on.

\end{description}

\resetcurrentobjects
\hypertarget{--doc-examples}{}

\chapter{Examples}


\section{\texttt{rhlapply}}


\subsection{Simple Example}

Take a sample of 100 iid observations Xi from N(0,1). Compute the mean of the eight closest neighbours to X1. This is repeated 1,000,000 times.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
nbrmean @PYGbe[@textless[]-] @PYGap[function](r){
  d @PYGbe[@textless[]-] matrix(rnorm(@PYGaS[200]),ncol@PYGbe[=]@PYGaS[2])
  orig @PYGbe[@textless[]-] d@PYGZlb[]@PYGaS[1],@PYGZrb[]
  ds @PYGbe[@textless[]-] sort(apply(d,@PYGaS[1],@PYGap[function](r) sqrt(sum((r@PYGbe[-]orig)@PYGbe[@textasciicircum[]]@PYGaS[2])))@PYGZlb[]@PYGaS[-1]@PYGZrb[])@PYGZlb[]@PYGaS[1]:@PYGaS[8]@PYGZrb[]
  mean(ds)
}
\end{Verbatim}

\textbf{One Machine}

\code{trials} is 1,000,000

\begin{Verbatim}[commandchars=@\[\]]
system.time({r @PYGbe[@textless[]-] sapply(@PYGaS[1]:trials, nbrmean)})
 user   system  elapsed
 @PYGaS[1603.414]    @PYGaS[0.127] @PYGaS[1603.789]
\end{Verbatim}

\textbf{Distributed, output to file}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
system.time({
  r @PYGbe[@textless[]-] rhlapply(
                trials, func@PYGbe[=]nbrmean,
                output.folder@PYGbe[=]@PYGaB["]@PYGaB[/test/one"])
})
user  system  elapsed
 @PYGaS[63.117]  @PYGaS[2.330] @PYGaS[179.747]
\end{Verbatim}

A \textbf{9x speed bump}. Note, the outputs are compressed, so Hadoop needs to decompress them. If native decompression libraries are not found, Hadoop uses java to decompress.


\subsection{Using Shared Files and Side Effects}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
h@PYGbe[=]rhlapply(length(simlist)
  ,func@PYGbe[=]@PYGap[function](r){
    @PYGaf[@#@# do something from data loaded from session.Rdata]
    pdf(@PYGaB["]@PYGaB[tmp/a.pdf"])
    plot(animage)
    dev.off()},
  configure@PYGbe[=]expression({
    load(@PYGaB["]@PYGaB[session.Rdata"])
  }),
  hadoop@PYGbe[=]list(mapred.map.tasks@PYGbe[=]@PYGaS[1000]),
  shared.files@PYGbe[=](@PYGaB["]@PYGaB[/tmp/session.Rdata"]))
\end{Verbatim}

Here \code{session.Rdata} is copied from HDFS to local temporary directories (making for faster reads). This
is a useful idiom for loading code that the \code{rhlapply} function might depend on. For example, assuming the image is not \emph{huge}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rhsave.image(@PYGaB["]@PYGaB[/tmp/myimage.Rdata"])
rhlapply(N,@PYGap[function](r) {
  object @PYGbe[@textless[]-] dataset@PYGZlb[]@PYGZlb[]r@PYGZrb[]@PYGZrb[]
  G(object)
},configure@PYGbe[=]expression({load(@PYGaB["]@PYGaB[myimage.Rdata"])}))
\end{Verbatim}

In the above example, I wish to apply the \code{G} to every element in \code{dataset}.


\section{\texttt{rhmr}}


\subsection{Word Count}

Generate the words, 1 word every line

\begin{Verbatim}[commandchars=@\[\]]
rhlapply(@PYGaS[10000],@PYGap[function](r) paste(sample(letters@PYGZlb[]@PYGaS[1]:@PYGaS[10]@PYGZrb[],@PYGaS[5]),collapse@PYGbe[=]@PYGaB["]@PYGaB["]),output.folder@PYGbe[=]'@PYGbe[/]tmp@PYGbe[/]words')
\end{Verbatim}

Word count using the sequence file

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
m @PYGbe[@textless[]-] @PYGap[function](key,word){
   list(list(key@PYGbe[=]word,value@PYGbe[=]@PYGaS[1]))
   }
 r @PYGbe[@textless[]-] @PYGap[function](key,value){
   value @PYGbe[@textless[]-] do.call(@PYGaB["]@PYGaB[sum"],value)
   @PYGap[return](list(list(key@PYGbe[=]key,value@PYGbe[=]value)))
  }
\end{Verbatim}

Run it

\begin{Verbatim}[commandchars=@\[\]]
rhmr(map@PYGbe[=]m,reduce@PYGbe[=]r,input.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/words"],output.folder@PYGbe[=]'@PYGbe[/]tmp@PYGbe[/]words.cnt'
   ,inputformat@PYGbe[=]'SequenceFileInputFormat',outputformat@PYGbe[=]'SequenceFileOutputFormat')
\end{Verbatim}


\subsection{Subset a file}

We can use this RHIPE to subset files.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
m @PYGbe[@textless[]-] @PYGap[function](key,val){
  @PYGap[if](condition.is.true(val))
    @PYGap[return](list(list(key@PYGbe[=]'',value@PYGbe[=]val)))
}
rhmr(mapper@PYGbe[=]m,reduce@PYGbe[=]@PYGap[function](){},
     input.folder@PYGbe[=]inf,output.folder@PYGbe[=]opf,
     hadoop@PYGbe[=]list(mapred.reduce.tasks@PYGbe[=]@PYGaS[0]))
\end{Verbatim}


\subsection{Covariance Matrix}

First create a file of 50 million rows with 100 columns.

\begin{Verbatim}[commandchars=@\[\]]
f @PYGbe[@textless[]-] @PYGap[function](k){
  @PYGap[return](rnorm(@PYGaS[100],@PYGaS[0],@PYGaS[1]))
}
rhlapply(@PYGaS[50]e6,f,output.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/bigd"],takeAll@PYGbe[=]F)
\end{Verbatim}

Now calculate the column sums, sum of squares and dot products which is sufficient to calculate correlations, for the first 100 columns.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
m @PYGbe[@textless[]-] @PYGap[function](k,v){
  v @PYGbe[@textless[]-] v@PYGZlb[]@PYGaS[1]:@PYGaS[100]@PYGZrb[]
  coln @PYGbe[@textless[]-] @PYGaS[1]:(length(v)@PYGbe[-]@PYGaS[1])
  tl @PYGbe[@textless[]-] length(v)
  ret @PYGbe[@textless[]-] sapply(coln,@PYGap[function](i){
    w @PYGbe[@textless[]-] v@PYGZlb[]i:tl@PYGZrb[]
    sums @PYGbe[@textless[]-] w@PYGZlb[]@PYGaS[1]@PYGZrb[]
    ssq @PYGbe[@textless[]-] w@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGbe[@textasciicircum[]]@PYGaS[2]
    dotprod @PYGbe[@textless[]-] w@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGbe[*]w@PYGZlb[]@PYGaS[2]:length(w)@PYGZrb[]
    list(key@PYGbe[=]as.integer(i),value@PYGbe[=]list(sums@PYGbe[=]sums,ssq@PYGbe[=]ssq,dotprod@PYGbe[=]dotprod))
  },simplify@PYGbe[=]F)
  @PYGap[return](ret)
}

r @PYGbe[@textless[]-] @PYGap[function](k,v){
  summs @PYGbe[@textless[]-] sum(do.call(@PYGaB["]@PYGaB[rbind"], lapply(v,@PYGap[function](r) r@$sums)))
  ssq @PYGbe[@textless[]-] sum(do.call(@PYGaB["]@PYGaB[rbind"], lapply(v,@PYGap[function](r) r@$ssq)))
  dotprod @PYGbe[@textless[]-] apply(do.call(@PYGaB["]@PYGaB[rbind"], lapply(v,@PYGap[function](r) r@$dotprod)),@PYGaS[2],sum)
  ret @PYGbe[=] list(list(key@PYGbe[=]k,value@PYGbe[=]list(sums@PYGbe[=]summs,ssq@PYGbe[=]ssq,dotprod@PYGbe[=]dotprod)))
  @PYGap[return](ret)
}

rhmr(map@PYGbe[=]m,reduce@PYGbe[=]r,combiner@PYGbe[=]T,input.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/bigd"],output.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/bigo"],
     inputformat@PYGbe[=]@PYGaB["]@PYGaB[SequenceFileInputFormat"],outputformat@PYGbe[=]@PYGaB["]@PYGaB[SequenceFileOutputFormat"])
\end{Verbatim}

The keys in the sequence file are the column numbers, each entry will have a contain value with the names sums,/ssq/ and dotprod which is enough to calculate correlations.

\begin{Verbatim}[commandchars=@\[\]]
suff @PYGbe[@textless[]-] rhsqallKV(@PYGaB["]@PYGaB[/tmp/bigo"],ignore@PYGbe[=]F)
\end{Verbatim}


\subsection{Naive K-Means Clustering}

Is there a need to cluster a billion row data set. Take a large sample, estimate the variances(of means) of the concerned columns and then take another sample controlling for the variance and cluster on the sample.

However, if you must,

Find the number of rows, we assume text input format.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
m @PYGbe[@textless[]-] @PYGap[function](key,value){
  @PYGaf[@#@# we use as.integer to save on the bytes sent.]
  @PYGap[return](list(list(key@PYGbe[=]as.integer(@PYGaS[1]),value@PYGbe[=]@PYGaS[1])))
}
r @PYGbe[@textless[]-] @PYGap[function](key,value){
  value @PYGbe[@textless[]-] sum(unlist(value))
  @PYGap[return](list(list(key@PYGbe[=]'count', value@PYGbe[=]value)))
}
rhmr(map@PYGbe[=]m,reducer@PYGbe[=]r, combiner@PYGbe[=]T, input.folder@PYGbe[=]X,output.folder@PYGbe[=]Y)
\end{Verbatim}

Read in the text file (broken up in part* files inside Y on the HDFS), there will be 1 row with key equal to count and the value is the number of rows.

Sample k values and makes these the centers c0

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
 @PYGaf[@#@#Only approximate sample, so increase it to get enough.]
 pct @PYGbe[@textless[]-] ncols @PYGbe[/] nrows@_of@_dataset@PYGbe[*]@PYGaS[2]
m @PYGbe[@textless[]-] @PYGap[function](key,value){
  y @PYGbe[@textless[]-] runif(@PYGaS[1])
  @PYGap[if](y @PYGbe[@textless[]] pct)
    @PYGap[return](list(list(key@PYGbe[=]@PYGaD[NULL],value@PYGbe[=]value)))
}
rhmr(map@PYGbe[=]m,reducer@PYGbe[=]@PYGap[function](){},preload@PYGbe[=]list(env@PYGbe[=]c('pct')),
     input.folder@PYGbe[=]X,output.folder@PYGbe[=]Y,
     hadoop@PYGbe[=]list(mapred.reduce.tasks@PYGbe[=]@PYGaS[0],rhipejob.kvsep@PYGbe[=]''))
system(@PYGaB["]@PYGaB[@$HADOOP/bin/hadoop dfs -cat /Y/p* @textgreater[] /tmp/centers.txt"])
read.table(@PYGaB["]@PYGaB[/tmp/centers.txt"])
@PYGaf[@#@# Create a matrix centers which has many columns as there are in there dataset]
@PYGaf[@#@# and k rows]
\end{Verbatim}

Find the distance of every point to the centers and emit the the center to which it is closest.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
 m @PYGbe[@textless[]-] @PYGap[function](r){
   r @PYGbe[@textless[]-] as.numeric(r)
   ret @PYGbe[@textless[]-] sapply(@PYGaS[1]:nrow(centers),@PYGap[function](k){
     m@PYGbe[=]centers@PYGZlb[]k,@PYGZrb[]
     c(k,(r@PYGZlb[]@PYGaS[2]@PYGZrb[]@PYGbe[-]m@PYGZlb[]@PYGaS[2]@PYGZrb[])@PYGbe[@textasciicircum[]]@PYGaS[2]@PYGbe[+](r@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGbe[-]m@PYGZlb[]@PYGaS[1]@PYGZrb[])@PYGbe[@textasciicircum[]]@PYGaS[2])
   },simplify@PYGbe[=]T)
   u @PYGbe[@textless[]-] which.min(ret@PYGZlb[]@PYGaS[2],@PYGZrb[])
   ret @PYGbe[@textless[]-] list( list( key@PYGbe[=]ret@PYGZlb[]@PYGaS[1],u@PYGZrb[], value @PYGbe[=] c(r,@PYGaS[1])))
   @PYGap[return](ret)
 }
 r @PYGbe[@textless[]-] @PYGap[function](key,value){
   value @PYGbe[@textless[]-] do.call(@PYGaB["]@PYGaB[rbind"],value)
   l @PYGbe[@textless[]-] list( list(key@PYGbe[=]key, value@PYGbe[=]apply(value,@PYGaS[2],sum)))
   @PYGap[return](l)
 }
rhmr(map@PYGbe[=]m,reduce@PYGbe[=]r,input.folder@PYGbe[=]@PYGaB["]@PYGaB[X"],output.folder@PYGbe[=]@PYGaB["]@PYGaB[Y"],combiner@PYGbe[=]T)
\end{Verbatim}

Read in the centers and see update centers (unless there is no change)

If finished iterating, assign rows to centers.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
assgn @PYGbe[@textless[]-] @PYGap[function](key,value){
  r @PYGbe[@textless[]-] as.numeric(value)
  ret @PYGbe[@textless[]-] sapply(@PYGaS[1]:nrow(centers),@PYGap[function](k){
    m @PYGbe[@textless[]-] centers@PYGZlb[]k,@PYGZrb[]
    c(k,(r@PYGZlb[]@PYGaS[2]@PYGZrb[]@PYGbe[-]m@PYGZlb[]@PYGaS[2]@PYGZrb[])@PYGbe[@textasciicircum[]]@PYGaS[2]@PYGbe[+](r@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGbe[-]m@PYGZlb[]@PYGaS[1]@PYGZrb[])@PYGbe[@textasciicircum[]]@PYGaS[2])
  },simplify@PYGbe[=]T)
  u @PYGbe[@textless[]-] which.min(ret@PYGZlb[]@PYGaS[2],@PYGZrb[])
  fret @PYGbe[@textless[]-] c(r,ret@PYGZlb[]@PYGaS[1],u@PYGZrb[])
  @PYGap[if] (runif(@PYGaS[1])@PYGbe[@textless[]]@PYGaS[0.2]) @PYGap[return](list((list(key@PYGbe[=]key,value@PYGbe[=]fret)))
                         }
  rhmr(map@PYGbe[=]assgn,red@PYGbe[=]@PYGap[function](){},input.folder@PYGbe[=]@PYGaB["]@PYGaB[X"],output.folder@PYGbe[=]@PYGaB["]@PYGaB[Y"],
       preload@PYGbe[=]list(env@PYGbe[=]'centers'),
       hadoop@PYGbe[=]list(mapred.reduce.tasks@PYGbe[=]@PYGaS[0]))
\end{Verbatim}


\subsection{Using a step == TRUE}

Like \code{tapply}, this calculates the sum of the second column in a text file

One can use a combiner, but my file is small and I did not bother.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
mapper @PYGbe[@textless[]-] @PYGap[function](key,r){
  x @PYGbe[@textless[]-] strsplit(r,@PYGaB["]@PYGaB[ +"])@PYGZlb[]@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGZrb[]
  ret @PYGbe[@textless[]-] list()
  ret@PYGZlb[]@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGZrb[] @PYGbe[@textless[]-] list(key @PYGbe[=] x@PYGZlb[]@PYGaS[1]@PYGZrb[], value @PYGbe[=] as.numeric(x@PYGZlb[]@PYGaS[2]@PYGZrb[]))
  @PYGap[return](ret)
}
reducer @PYGbe[@textless[]-] @PYGap[function](key,r){
  r @PYGbe[@textless[]-] do.call(@PYGaB["]@PYGaB[rbind"],r)
  v@PYGbe[=]apply(r,@PYGaS[2],sum)
  ret @PYGbe[@textless[]-] list()
  ret@PYGZlb[]@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGZrb[] @PYGbe[@textless[]-] list(key@PYGbe[=]key,value@PYGbe[=]v)
  ret
}
rhmr(input.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/wc"],output.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/rand.out"],map@PYGbe[=]mapper,reduce@PYGbe[=]reduce)
\end{Verbatim}

Doing the same with \code{step} equal to TRUE

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
reduce @PYGbe[@textless[]-] @PYGap[function](key,value){
@PYGaf[@#@#Note the global assignment, the assignment is now permanent]
@PYGaf[@#@#which is required since this function will be called repeatedly]
  @PYGap[if](red.status@PYGbe[==]@PYGaS[1]) sums @PYGbe[@textless[]]@PYGbe[@textless[]-] @PYGaS[1]
  @PYGap[else] @PYGap[if] (red.status@PYGbe[==]@PYGaS[0]){
    sums @PYGbe[@textless[]]@PYGbe[@textless[]-] sums@PYGbe[+]value
  }else{
    @PYGaf[@#@#red.status==-1]
    list(list(key@PYGbe[=]key,value@PYGbe[=]sums))
  }
}
rhmr(input.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/rand2"],output.folder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/rand.out"],map@PYGbe[=]mapper,reduce@PYGbe[=]reduce,step@PYGbe[=]T)
\end{Verbatim}

\resetcurrentobjects
\hypertarget{--doc-FAQ}{}

\chapter{FAQ}
\begin{enumerate}
\item {} 
Local Testing?

\end{enumerate}

Easily enough. In \code{rhmr} or \code{rhlapply}, set \code{mapred.job.tracker} to
`local' in the \code{hadoop.options} option of the respective command. This will
use the local jobtracker to run your commands.

However keep in mind,
\code{shared.files} will not work, i.e those files will not be copied to the
working directory and side effect files will not be copied back.
\begin{enumerate}
\item {} 
Speed?

\end{enumerate}

Not so good. A lot of data is transferred and because of R's serialization and
my effort to allow \emph{any} R data type to be transferred, about 6 times more data
is transferred in the RHIPE wordcount example(see Mainpage )
compared to the Java version. As such, this was about 5 times slower for 9MM
lines.
\begin{enumerate}
\item {} 
Other Approaches?

\end{enumerate}
\begin{itemize}
\item {} 
I will try using Unix pipes to feed data to R instead of Rserve.

\item {} 
Also, I will just settle for raw,numeric, complex, character vectors and lists
(whose elements are one of the aforementioned types) in conjunction with
Protocol Buffers. This does reduce the transparency to the R user in that
he/she will now have to serialize other structures (e.g objects like \code{xyplot}
objects or objects with special attributes

\end{itemize}


\renewcommand{\indexname}{Module Index}
\printmodindex
\renewcommand{\indexname}{Index}
\printindex
\end{document}
