% Generated by Sphinx.
\documentclass[letterpaper,10pt,english]{manual}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}


\title{rhipe Documentation}
\date{May 06, 2010}
\release{0.59}
\author{Saptarshi Guha}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex
\makemodindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\def\PYG@tok@gd{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\def\PYG@tok@gu{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\def\PYG@tok@gt{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\def\PYG@tok@gs{\let\PYG@bf=\textbf}
\def\PYG@tok@gr{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\def\PYG@tok@cm{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@vg{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@m{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mh{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@cs{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\colorbox[rgb]{1.00,0.94,0.94}{##1}}}
\def\PYG@tok@ge{\let\PYG@it=\textit}
\def\PYG@tok@vc{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@il{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@go{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\def\PYG@tok@cp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@gi{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\def\PYG@tok@gh{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\def\PYG@tok@ni{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\def\PYG@tok@nl{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\def\PYG@tok@nn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@no{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\def\PYG@tok@na{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@nb{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@nd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\def\PYG@tok@ne{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nf{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\def\PYG@tok@si{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\def\PYG@tok@s2{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@vi{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@nt{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\def\PYG@tok@nv{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@s1{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@gp{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@sh{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@ow{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@sx{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@bp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c1{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@kc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@mf{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@err{\def\PYG@bc##1{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{##1}}}
\def\PYG@tok@kd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@ss{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\def\PYG@tok@sr{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\def\PYG@tok@mo{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mi{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@kn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@o{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\def\PYG@tok@kr{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@s{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@kp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@w{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\def\PYG@tok@kt{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\def\PYG@tok@sc{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sb{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@k{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@se{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sd{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents



Mainpage

\resetcurrentobjects
\hypertarget{--doc-installation}{}

\chapter{Setting up RHIPE}


\section{Requirements}
\begin{enumerate}
\item {} 
\emph{Protobuffers}

RHIPE uses Google's Protobuf library for serialization. This(the C/C++
libraries) must be installed on \emph{all} machines (master/workers). Get
Protobuffers from \href{http://code.google.com/p/protobuf/}{http://code.google.com/p/protobuf/}. RHIPE already has the
protobuf jar file inside it.
\begin{description}
\item[Non Standard Locations]
If installing protobuf to a non standard location, update the
PKG\_CONFIG\_PATH variable, e.g

\begin{Verbatim}[commandchars=\\\{\}]
export PKG\PYGZus{}CONFIG\PYGZus{}PATH\PYG{o}{=}\PYG{p}{@$}PKG\PYGZus{}CONFIG\PYGZus{}PATH:\PYG{p}{@$}CUSTROOT\PYG{o}{/}lib\PYG{o}{/}pkgconfig\PYG{o}{/}
\end{Verbatim}

\end{description}

\item {} 
\emph{R} , tested on 2.8

\item {} 
\emph{rJava} The R package needs rJava.

\end{enumerate}

Tested on RHEL Linux, Mac OS 10.5.5 (Leopard).
Does not work on Snow Leopard


\section{Installation}

Rhipe requires the following environment variables

\begin{Verbatim}[commandchars=\\\{\}]
HADOOP\PYG{o}{=}location of Hadoop installation
HADOOP\PYGZus{}LIB\PYG{o}{=}location of lib jar files included with Hadoop\PYG{p}{,} usually
\PYG{p}{@$}HADOOP\PYG{o}{/}lib
HADOOP\PYGZus{}CONF\PYGZus{}DIR\PYG{o}{=}location of Hadoop conf folder\PYG{p}{,} \PYG{p}{(}usually \PYG{p}{@$}HADOOP\PYG{o}{/}conf\PYG{p}{)}
\end{Verbatim}

On every machine

\begin{Verbatim}[commandchars=\\\{\}]
R CMD INSTALL Rhipe\PYGZus{}VERSION.tar.gz
\end{Verbatim}

To load it

\begin{Verbatim}[commandchars=\\\{\}]
library\PYG{p}{(}Rhipe\PYG{p}{)}
\end{Verbatim}

\resetcurrentobjects
\hypertarget{--doc-rhlapply}{}

\chapter{The \texttt{rhlapply} Command}


\section{Introduction}

\code{rhapply} applies a user defined function to the elements of a given
R list or the function can be run over the set of numbers from 1 to
n. In the former case the list is written to a sequence file,whose length is the
default setting of \code{rhwrite}.

Running a hundreds of thousadands of seperate trials
can be terribly inefficient, instead consider grouping them, i.e set
\code{mapred.max.tasks} to a value much smaller than the length of the
list.


\section{Return Value}

\code{rhlapply} returns a list, the names of which is equal to the names
of the input list (if given).


\section{Function Usage}

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
rhlapply \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(} ll\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}
                     fun\PYG{p}{,}
                     ifolder\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{"}\PYG{p}{,}
                     ofolder\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{"}\PYG{p}{,}
                     readIn\PYG{o}{=}\PYG{k+kVariable}{T}\PYG{p}{,}
                     inout\PYG{o}{=}c\PYG{p}{(}\PYG{l+s}{'}\PYG{l+s}{lapply'}\PYG{p}{,}\PYG{l+s}{'}\PYG{l+s}{sequence'}\PYG{p}{)}
                     mapred\PYG{o}{=}list\PYG{p}{(}\PYG{p}{)}
                     setup\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}jobname\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{rhlapply"}\PYG{p}{,}doLocal\PYG{o}{=}\PYG{k+kVariable}{F}\PYG{p}{,}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{l+m}{.}
                     \PYG{p}{)}
\end{Verbatim}

Description follows
\begin{description}
\item[\code{ll}]
The list object, optional. Applies \code{fun} to \code{ll{[}{[}i{]}{]}} .
If instead \code{ll} is a numeric, applies \code{fun} to each element of
\code{seq(1,ll)}. If not given, must provide a value for \code{ifolder}

\item[\code{fun}]
A function that takes only one argument.

\item[\code{ifolder}]
If \code{ll} is null, provide a source here. Also change the value of
\code{inout{[}1{]}} to either \code{text} or \code{sequence}.

\item[\code{readIn}]
The results are stored in a temporary sequence file on the DFS which is
deleted. Should the results be returned in a list? Default is TRUE. For
large number of output key-values (e.g 1MM) set this to FALSE, using the
default options to \code{rhread} is extremely slow.

\item[\code{ofolder}]
If given the results are written to this folder and not deleted. If not,
they are written to temporary folder, read back in (assuming \code{readIn}
is TRUE) and deleted.

\item[\code{N}]
The number of task to create, i.e the mapred.map.tasks and is passes onto the \code{rhwrite} function

\item[\code{mapred}]
Options passed onto \code{rhmr}

\item[\code{setup}]
And expression that is called before running \code{func}. Called once per
JVM.

\item[\code{aggr}]
A function (default is NULL) to aggregate results. If NULL (default), every list element is written to disk.
This can be difficult to read back into R (especially when one has nearly 1MN trials, R has to combine a list
of 1MN elements!). \code{aggr} is a function that takes one argument a list of values, each value being the result
apply the user function to an element of the input list. E.g. if \code{fun} returns a data frame, one could write

\end{description}

\begin{Verbatim}[commandchars=\\\{\}]
aggr\PYG{o}{=}\PYG{k+kr}{function}\PYG{p}{(}x\PYG{p}{)} do.call\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{rbind"}\PYG{p}{,}x\PYG{p}{)}

and the result of rhlapply will be one big data frame.
\end{Verbatim}
\begin{description}
\item[\code{doLocal}]
Default is \code{F}. Sent to \code{rhread}

\item[\code{...}]
passed onto RHMR.

\end{description}


\subsection{RETURN}

An object that is passed onto \code{rhex}.


\subsection{IMPORTANT}

The object passed to rhex has variable called \code{rhipe\_command} which is the
command of the program that Hadoop sends information to. In case the client
machine's (machine from which commands are being sent ) R installation is different from the
tasktrackers' R installation the RHIPE command runner wont be found. For example
suppose my cluster is linux and my client is OS X , then the \code{rhipe\_command}
variable will reflect the location of the rhipe command runner on OS X and not
that of the taskttrackers(Linux) R distribution.

There are two ways to fix this
a) after \code{z \textless{}- rhlapply(...)} change \code{r{[}{[}1{]}{]}{[}{[}1{]}{]}\$rhipe\_command} to the
value it should be on the tasktrackers.

or

b) set the environment variable \code{RHIPECOMMAND} on each of tasktrackers. RHIPE
java client will read this first before reading the above variable.

\resetcurrentobjects
\hypertarget{--doc-rhmr}{}

\chapter{The \texttt{rhmr} Command}


\section{Introduction}

The \code{rhmr} command runs a general mapreduce program using user supplied map
and reduce commands.


\section{Return Value}

In general a set of files on the Hadoop Distributed File System. It can be of
Text Format or a Sequence file format. In case of the latter, the key and values
can be any R data structure.


\section{Function}

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
rhmr \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}map\PYG{p}{,}reduce\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}
         combiner\PYG{o}{=}\PYG{k+kVariable}{F}\PYG{p}{,}
         setup\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}
         cleanup\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}
         ofolder\PYG{o}{=}\PYG{l+s}{'}\PYG{l+s}{'}\PYG{p}{,}
         ifolder\PYG{o}{=}\PYG{l+s}{'}\PYG{l+s}{'}\PYG{p}{,}
         inout\PYG{o}{=}c\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{text"}\PYG{p}{,}\PYG{l+s}{"}\PYG{l+s}{text"}\PYG{p}{)}\PYG{p}{,}
         mapred\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}
         shared\PYG{o}{=}c\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
         jarfiles\PYG{o}{=}c\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
         copyFiles\PYG{o}{=}\PYG{k+kVariable}{F}\PYG{p}{,}
         opts\PYG{o}{=}rhoptions\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}jobname\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{"}\PYG{p}{)}
\end{Verbatim}
\begin{description}
\item[\code{map}]
A map expression, not a function. The map expression can expect a list of keys in \code{map.keys} and list of values in \code{map.values}.

\item[\code{reduce}]
Can be null if only a map job. If not,reduce should be an expression with three attributes
\begin{description}
\item[\code{pre}]
Called for a new key, but no values have been read. The key is present in \code{reduce.key}.

\item[\code{reduce}]
Called for reducing the incoming values. The values are in a list called \code{reduce.values}

\item[\code{post}]
Called when all the values have been sent.

\end{description}

\item[\code{combiner}]
Uses a combiner if TRUE. If so, then \code{reduce.values} present in the \code{reduce\$reduce} expression will be a \emph{subset} of values.The reducer algorithm should be able process input emitted from map \emph{or} reduce.

\item[\code{setup}]
An expression that can be called to setup the environment. Called once for every task.
It can be a list of two attributes \code{map} and \code{reduce} which are expressions to be run in the map and reduce stage. If a single expression then that is run for both map and reduce

\item[\code{cleanup}]
Same as for \code{setup}, run when all work for a task is complete.

\item[\code{ifolder}]
A folder or file to be processed. Can be a vector of strings.

\item[\code{ofolder}]
The folder to store output in. Side effects will be copied here.

\item[\code{inout{}`}]\begin{description}
\item[A vector of input type and output type.]\begin{description}
\item[\code{text}]
indicates Text Format. Use \code{mapred.field.separator} to seperate the elements of a vector.

\end{description}

\item[\code{sequence}]
is a sequence format. Outputs in this form /can/ be used as an input.

\item[\code{binary}]
is a simple binary format consisting of key-length, key data, value-length, value data where the lengths are integers in network order. Though \emph{much} faster than sequence in terms of reading in data, it \emph{cannot} be used an input to a map reduce operation.

\item[\code{map}]
\emph{Only as OutputFormat} ! That is, map can only be the second element of \code{inout}. If so, the output part files will be directories, each containing a data and an index file. If the reducer writes the same key as the one received then using the function \code{rhgetkey}, specifying the get and the output folder part files , one can use the output as a hash table (do keep the keys small then). However, if the keys are changed before being written (using rhcollect), the order is lost and even though one can still use the individual part file as a Map file reader, the part file containing the key needs to be known (as opposed to just specifying the directory of part files). To remedy this just run a identity map job converting map input to map output (see \code{rhM2M} and \code{rhS2M}).
Map Output formats can be used an input format. Use the function \code{rhmap.sq} on a directory of map part files e.g \code{rhmap.sq("/tmp/out/p*)}, this will return a vector of paths pointing to the \emph{data} files in each of the part folders (the folders also contain index files, which can't be used as sequence file input to Hadoop, so these have to be filtered).

\item[\code{shared}]
A vector of files on the HDFS that will be copied to the working directory of the R program. These files can then be loaded as easily as \code{load(filename)} (removed leading path)

\end{description}

\item[\code{jarfiles}]
Copy jar files if required. Experimental, probably doesn't work.

\item[\code{copyFiles}]
For side effects to be copied back to the DFS, set this to TRUE, otherwise they wont be copied.

\item[\code{mapred}]
Set Hadoop options here and RHIPE options.

\item[\code{jobname}]
the jobname, if not given, then current date and time is the job title.

\end{description}


\section{RHIPE Options}
\begin{description}
\item[\textbf{rhipe\_stream\_buffer}]
The size of the STDIN buffer used to write data to the R process(in bytes)
\emph{default:} 10*1024 bytes

\item[\textbf{mapred.textoutputformat.separator}]
The text that seperates the key from value when \code{inout{[}2{]}} equals text.
\emph{default:} Tab

\item[\textbf{mapred.field.separator}]
The text that seperates fields when \code{inout{[}2{]}} equals text.
\emph{default:} Space

\item[\textbf{rhipe\_reduce\_buff\_size}]
The maximum length of \code{reduce.values}
\emph{default:} 10,000

\item[\textbf{rhipe\_map\_buff\_size}]
The maximum length of \code{map.values} (and \code{map.keys})
\emph{default:} 10,000

\end{description}


\section{Status, Counters and Writing Output}


\subsection{Status}

To update the status use \code{rhstatus} which takes a single string e.g \code{rhstatus("Nice")}
This will also indicate progress.


\subsection{Counter}

To update the counter C in the group G with a number N, user \code{rhcounter(G,C,N)}
where C and G are strings and N is a number. However, C and G can be atomic vectors and they will be converted to strings.
Previously a ``,'' in C or G would upset Hadoop, but not with version 0.52 onwards.
The values will be returned to the R session.
Output
\textasciicircum{}\textasciicircum{}\textasciicircum{}\textasciicircum{}\textasciicircum{}\textasciicircum{}
To output data use \code{rhcollect(KEY,VALUE)} where KEY and VALUE are R objects that can be serialized by \code{rhsz} (see the misc page). If one needs to send across complex R objects e.g the KEY is a function, do something like \code{rhcollect(serialize(KEY,NULL),VALUE)}


\section{Side Effect files}

Files written to \code{tmp/} (no leading slash !) e.g \code{pdf("tmp/x.pdf")} will be copied to the output folder.


\section{Mapreduce Options}

Many mapreduce configuration variables are stored in the environment. To get the value use \code{Sys.getenv()}, e.g in the map stage, to find out the name of the current input file, use \code{Sys.getenv('mapred.input.file')} .


\section{IMPORTANT}

The object passed to rhex has variable called \code{rhipe\_command} which is the
command of the program that Hadoop sends information to. In case the client
machine's (machine from which commands are being sent ) R installation is different from the
tasktrackers' R installation the RHIPE command runner wont be found. For example
suppose my cluster is linux and my client is OS X , then the \code{rhipe\_command}
variable will reflect the location of the rhipe command runner on OS X and not
that of the taskttrackers(Linux) R distribution.

There are two ways to fix this
a) after \code{z \textless{}- rhmr(...)} change \code{r{[}{[}1{]}{]}\$rhipe\_command} to the
value it should be on the tasktrackers.

or

b) set the environment variable \code{RHIPECOMMAND} on each of tasktrackers. RHIPE
java client will read this first before reading the above variable.

\resetcurrentobjects
\hypertarget{--doc-rhmisc}{}

\chapter{Miscellaneous Commands}


\section{Introduction}

This is a list of supporting functions for reading, writing sequence files and
manipulating files on the Hadoop Distributed File System (HDFS).


\section{Serialization}


\subsection{rhsz}

\begin{Verbatim}[commandchars=\\\{\}]
rhsz \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}object\PYG{p}{)}
\end{Verbatim}

Serializes a given R object. Currently the only objects that can be serialized
are vectors of Raws,Numerics, Integers, Strings(including NA), Logical(including NA)
and lists of these and lists of lists of these. Attributes are copied to(e.g
names attributes). It appears objects like matrices, factors also get serialized
and unserialized sucessfully.


\subsection{rhuz}

\begin{Verbatim}[commandchars=\\\{\}]
rhuz \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}object\PYG{p}{)}
\end{Verbatim}

Unserializes a raw object returned from \code{rhsz}


\section{HDFS Related}


\subsection{rhload}

\begin{Verbatim}[commandchars=\\\{\}]
rhload \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}file\PYG{p}{,}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{p}{)}
\end{Verbatim}

Loads an R data set stored on the DFS.


\subsection{rhsave}

\begin{Verbatim}[commandchars=\\\{\}]
rhsave \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{p}{,} file\PYG{p}{)}
\end{Verbatim}

Saves the objects in \code{...} to \code{file} on the HDFS. All other options are
passed onto the R function \code{save}


\subsection{rhsave.image}

\begin{Verbatim}[commandchars=\\\{\}]
rhsave.image \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{p}{,} file\PYG{p}{)}
\end{Verbatim}

Same as R's \code{save.image}, except that the file goes to the HDFS.


\subsection{rhcp}

\begin{Verbatim}[commandchars=\\\{\}]
rhcp \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}ifile\PYG{p}{,}ofile\PYG{p}{)}
\end{Verbatim}

Copies a \code{ifile} to \code{ofile} on the HDFS, i.e. both files must be present on the HDFS.


\subsection{rhmv}

\begin{Verbatim}[commandchars=\\\{\}]
rhmv \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}ifile\PYG{p}{,}ofile\PYG{p}{)}
\end{Verbatim}

Moves \code{ifile} to \code{ofile} on the HDFS (and deletes \code{ifile}).


\subsection{rhput}

\begin{Verbatim}[commandchars=\\\{\}]
rhput \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}src\PYG{p}{,}dest\PYG{p}{,}deleteDest\PYG{o}{=}\PYG{k+kc}{TRUE}\PYG{p}{)}
\end{Verbatim}

Copies the file in \code{src} to the \code{dest} on the HDFS, deleting destination if
\code{deleteDest} is TRUE.


\subsection{rhget}

\begin{Verbatim}[commandchars=\\\{\}]
rhget \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}src\PYG{p}{,}dest\PYG{p}{)}
\end{Verbatim}

Copies \code{src{}`{}`(on the HDFS) to {}`{}`dest} on the local. If \code{src} is a directory and \code{dest} exists,
\code{src} is copied inside \code{dest{}`{}`(i.e a folder inside {}`{}`dest}).If not(i.e
\code{dest} does not exist), \code{src}`s contents is copied to a new folder called
\code{dest}.  If \code{src} is a file, and \code{dest} is a directory \code{src} is copied
inside \code{dest} . If \code{dest} does not exist, it is copied to that file

Wildcards allowed

OVERWRITES!


\subsection{rhls}

\begin{Verbatim}[commandchars=\\\{\}]
rhls \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}dir\PYG{p}{,}recur\PYG{o}{=}\PYG{k+kc}{FALSE}\PYG{p}{)}
\end{Verbatim}

Lists the path at \code{dir}. Wildcards allowed. Use \code{recur} (FALSE/TRUE) to not recurse or to recurse.


\subsection{rhdel}

\begin{Verbatim}[commandchars=\\\{\}]
rhdel \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}dir\PYG{p}{)}
\end{Verbatim}

Deletes file(s) at/in \code{dir}. Wildcards allowed.


\subsection{rhwrite}

\begin{Verbatim}[commandchars=\\\{\}]
rhwrite \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}lo\PYG{p}{,}f\PYG{p}{,}n\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{l+m}{.}\PYG{p}{)}
\end{Verbatim}

Writes the list \code{lo}  to the file \code{f}. \code{n} is the number of sequence files
to split the list into.  The default value of \code{n} is
\code{mapred.map.tasks} * \code{mapred.tasktracker.map.tasks.maximum} .


\subsection{rhread}

\begin{Verbatim}[commandchars=\\\{\}]
rhread \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}files\PYG{p}{,}max\PYG{o}{=}\PYG{l+m}{-1}\PYG{p}{,}type\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{sequence"}\PYG{p}{,}verbose\PYG{o}{=}\PYG{k+kVariable}{T}\PYG{p}{)}
\end{Verbatim}

Reads files(s) from \code{files} (which could be a directory). Wildcards allowed.

If \code{verbose} is True, information is displayed (useful when reading many
files)

If \code{max} is positive, \code{max} key-value pairs will be read.

Set \code{type} to ``map'' if the directory \code{files} contains map folders.


\subsection{rhmerge}

\begin{Verbatim}[commandchars=\\\{\}]
rhmerge\PYG{p}{(}inr\PYG{p}{,}ou\PYG{p}{)}
\end{Verbatim}

\code{inr} can have wildcards. Usually used to merge all files in a directory into one file \code{ou} on the local file system.


\subsection{rhreadBin}

\begin{Verbatim}[commandchars=\\\{\}]
rhreadBin \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}filename\PYG{p}{,} max\PYG{o}{=}as.integer\PYG{p}{(}\PYG{l+m}{-1}\PYG{p}{)}\PYG{p}{,} bf\PYG{o}{=}as.integer\PYG{p}{(}\PYG{l+m}{0}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

Reads data outputed in `binary' form. \code{max} is the maximum number to read, -1
is all. \code{bf} is the read buffer, 0 implies the os specified default \code{BUFSIZ}


\section{Map Files}


\subsection{rhS2M}

\begin{Verbatim}[commandchars=\\\{\}]
rhS2M \PYG{o}{@textless[]-} \PYG{k+kr}{function} \PYG{p}{(}files\PYG{p}{,} ofile\PYG{p}{,} dolocal \PYG{o}{=} \PYG{k+kVariable}{T}\PYG{p}{,} ignore.stderr \PYG{o}{=} \PYG{k+kVariable}{F}\PYG{p}{,} verbose \PYG{o}{=} \PYG{k+kVariable}{F}\PYG{p}{)}
\end{Verbatim}

Converts the sequence files specified by \code{files} and places them in
destination \code{ofile}. If \code{dolocal} is True the conversion is done on the
local machine, otherwise over the cluster (which is much faster for anything
greater than hundreds of megabytes). If \code{ignore.stderr} is True, the mapreduce
output is displayed on the R console. e.g

\begin{Verbatim}[commandchars=\\\{\}]
rhS2m\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{/tmp/so/p*"}\PYG{p}{,}\PYG{l+s}{"}\PYG{l+s}{/tmp/so.map"}\PYG{p}{,}dolocal\PYG{o}{=}\PYG{k+kVariable}{F}\PYG{p}{)}
\end{Verbatim}


\subsection{rhM2M}

\begin{Verbatim}[commandchars=\\\{\}]
rhM2M \PYG{o}{@textless[]-} \PYG{k+kr}{function} \PYG{p}{(}files\PYG{p}{,} ofile\PYG{p}{,} dolocal \PYG{o}{=} \PYG{k+kVariable}{T}\PYG{p}{,} ignore.stderr \PYG{o}{=} \PYG{k+kVariable}{F}\PYG{p}{,} verbose \PYG{o}{=} \PYG{k+kVariable}{F}\PYG{p}{)}
\end{Verbatim}

Same as S2M, except it converts a group of Map files to Map files.Why?
Consider a mapreduce job that outputs modified keys in the reduce part, i.e the
reduce receives key K0 but emits f(K0), where f(K0) \textless{}\textgreater{} K0, the result of this
the keys in the reduce output part files wont be sorted even though the K0 are
sorted.

So, if the reducer emits K0, the output part files constitute a valid collection
of sorted map files. If the reducer emits f(K0), this does not hold any
more. Running \code{rhM2M} on this output produces another output in which the keys
are now sorted (i.e we just run an identity mapreduce emitting f(K0), though now
the input to the reducers are f(K0)).

To specify the input files, it is not enough to specify the directory
containing the part files, because the part files are directories which contain
a sequence file and a non sequence file. Specifying the list of directories to a
mapreduce job will cause it to fail when it reads the non-map file.

Use \code{rhmap.sqs} .


\subsection{rhgetkey}

\begin{Verbatim}[commandchars=\\\{\}]
rhgetkey \PYG{o}{@textless[]-} \PYG{k+kr}{function} \PYG{p}{(}keys\PYG{p}{,} paths\PYG{p}{,} sequence\PYG{o}{=}\PYG{k+kc}{NULL}\PYG{p}{,}skip\PYG{o}{=}\PYG{l+m}{0}\PYG{p}{,}ignore.stderr \PYG{o}{=} \PYG{k+kVariable}{T}\PYG{p}{,} verbose \PYG{o}{=} \PYG{k+kVariable}{F}\PYG{p}{)}
\end{Verbatim}

Given a list of keys and vector of  map directories (e.g /tmp/ou/mapoutput/p*''),
returns a list of key,values. If sequence is a string, the output key,values will be written to the sequence files on the DFS(the values will not be read into R).
Set skip to larger(integr) values to prevent reading in all keys of the table - slower to find your key, but can search a much large database.

\resetcurrentobjects
\hypertarget{--doc-ec2}{}

\chapter{Using RHIPE on EC2}


\section{Introduction}

RHIPE also works on EC2 using Cloudera's scripts. Let me demonstrate


\subsection{Download}

The Cloudera scripts can be found at \href{http://archive.cloudera.com/docs/\_getting\_started.html}{http://archive.cloudera.com/docs/\_getting\_started.html}

Follow the instructions to test your working EC2 installation.


\subsection{Using RHIPE on EC2}
\begin{enumerate}
\item {} 
You need to create an entry in your \emph{\textasciitilde{}/.hadoop-ec2/ec2-clusters.cfg}, e.g.

\end{enumerate}

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{o}{[}test2\PYG{o}{]}
\PYG{n+nv}{ami}\PYG{o}{=}ami-6159bf08 \PYG{c}{@# Fedora 32 bit instance}
\PYG{n+nv}{instance\PYGZus{}type}\PYG{o}{=}c1.medium
\PYG{n+nv}{key\PYGZus{}name}\PYG{o}{=}saptarshiguha \PYG{c}{@#@# Your key name}
\PYG{n+nv}{availability\PYGZus{}zone}\PYG{o}{=}us-east-1c
\PYG{n+nv}{private\PYGZus{}key}\PYG{o}{=}PATH\PYGZus{}TO\PYGZus{}PRIVATE\PYGZus{}KEY
\PYG{n+nv}{ssh\PYGZus{}options}\PYG{o}{=} -i @%\PYG{o}{(}private\PYGZus{}key\PYG{o}{)}s -o \PYG{n+nv}{StrictHostKeyChecking}\PYG{o}{=}no
\PYG{n+nv}{user\PYGZus{}data\PYGZus{}file}\PYG{o}{=}the file you download in step 2
\end{Verbatim}

In particular, RHIPE only works with 32/64 bit Fedora instance types, so choose those AMIs.
\begin{enumerate}
\item {} 
Download this file( \href{http://github.com/saptarshiguha/RHIPE/blob/master/code/hadoop-ec2-init-remote.sh}{http://github.com/saptarshiguha/RHIPE/blob/master/code/hadoop-ec2-init-remote.sh} ) and replace the file of the same name (it is in the Cloudera distribution). This file contains one extra shell function to install code RHIPE requires: R, Google's protobuf and RHIPE

\item {} 
Now start your cluster

\end{enumerate}

\begin{Verbatim}[commandchars=\\\{\}]
python hadoop-ec2 launch-cluster --env \PYG{n+nv}{REPO}\PYG{o}{=}testing --env \PYG{n+nv}{HADOOP\PYGZus{}VERSION}\PYG{o}{=}0.20 test2 3
\end{Verbatim}

The number (3) must be greater than 1.
\begin{enumerate}
\item {} 
\emph{Wait}, till you it completely finishes booting up (the cloudera scripts tell you the url of the jobtracker). Login to the cluster

\end{enumerate}

\begin{Verbatim}[commandchars=\\\{\}]
python hadoop-ec2 login test2
\end{Verbatim}
\begin{enumerate}
\item {} 
Start \emph{R}, and try the following

\end{enumerate}

\begin{Verbatim}[commandchars=\\\{\}]
library\PYG{o}{(}Rhipe\PYG{o}{)}
z @textless[]- rhlapply\PYG{o}{(}10,runif\PYG{o}{)}
\PYG{c}{@#@# Runs on a local machine(i.e the master)}
rhex\PYG{o}{(}z,changes\PYG{o}{=}list\PYG{o}{(}mapred.job.tracker\PYG{o}{=}\PYG{l+s+s1}{'local'}\PYG{o}{)}\PYG{o}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
library\PYG{o}{(}Rhipe\PYG{o}{)}
\PYG{c}{@#@# Runs on the cluster}
z @textless[]- rhlapply\PYG{o}{(}10,runif\PYG{o}{)}
rhex\PYG{o}{(}z\PYG{o}{)}
\end{Verbatim}

6. Consider the more involved problem of bootstrapping. See this question posed on the
R-HPC mailing list (\href{http://permalink.gmane.org/gmane.comp.lang.r.hpc/221}{http://permalink.gmane.org/gmane.comp.lang.r.hpc/221}).
Using Rhipe( chunksize (see the posting) is 1000 per task which results in 100 tasks)

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
y @textless[]- iris\PYG{o}{[}which\PYG{o}{(}iris\PYG{o}{[},5\PYG{o}{]} !\PYG{o}{=} \PYG{l+s+s2}{"setosa"}\PYG{o}{)}, c\PYG{o}{(}1,5\PYG{o}{)}\PYG{o}{]}
rhsave\PYG{o}{(}y,file\PYG{o}{=}\PYG{l+s+s2}{"/tmp/tmp.Rdata"}\PYG{o}{)}

\PYG{c}{@#@# The function 'f' depends on 'x' so we must save it}
\PYG{c}{@#@# using rhsave and then load it in the setup}

setup @textless[]- expression\PYG{o}{(}\PYG{o}{\PYGZob{}}
   load\PYG{o}{(}\PYG{l+s+s2}{"tmp.Rdata"}\PYG{o}{)}
  \PYG{o}{\PYGZcb{}}\PYG{o}{)}

f@textless[]- \PYG{k}{function}\PYG{o}{(}i\PYG{o}{)}\PYG{o}{\PYGZob{}}
   ind @textless[]- sample\PYG{o}{(}100, 100, \PYG{n+nv}{replace}\PYG{o}{=}TRUE\PYG{o}{)}
   result1 @textless[]- glm\PYG{o}{(}y\PYG{o}{[}ind,2\PYG{o}{]}@textasciitilde[]y\PYG{o}{[}ind,1\PYG{o}{]}, \PYG{n+nv}{family}\PYG{o}{=}binomial\PYG{o}{(}logit\PYG{o}{)}\PYG{o}{)}
   \PYG{k}{return}\PYG{o}{(}structure\PYG{o}{(}coefficients\PYG{o}{(}result1\PYG{o}{)}, \PYG{n+nv}{names}\PYG{o}{=}NULL\PYG{o}{)}\PYG{o}{)}
\PYG{o}{\PYGZcb{}}

z @textless[]- rhlapply\PYG{o}{(}100000L,f,shared\PYG{o}{=}\PYG{l+s+s2}{"/tmp/tmp.Rdata"},setup\PYG{o}{=}setup,
             \PYG{n+nv}{mapred}\PYG{o}{=}list\PYG{o}{(}mapred.map.tasks\PYG{o}{=}100000L/1000
               ,mapred.reduce.tasks\PYG{o}{=}5\PYG{o}{)}\PYG{o}{)}

g @textless[]- rhex\PYG{o}{(}z\PYG{o}{)}
g1 @textless[]- \PYG{k}{do}.call\PYG{o}{(}\PYG{l+s+s2}{"rbind"},lapply\PYG{o}{(}g,function\PYG{o}{(}r\PYG{o}{)} r\PYG{o}{[}\PYG{o}{[}2\PYG{o}{]}\PYG{o}{]}\PYG{o}{)}\PYG{o}{)}
g2 @textless[]- cbind\PYG{o}{(}unlist\PYG{o}{(}lapply\PYG{o}{(}g,function\PYG{o}{(}r\PYG{o}{)} r\PYG{o}{[}\PYG{o}{[}1\PYG{o}{]}\PYG{o}{]}\PYG{o}{)}\PYG{o}{)},g1\PYG{o}{)}
\end{Verbatim}

I used 3 c1.xlarge nodes(each \$0.68/hr). This took 2 minutes and  5 seconds to run and another minute to read the data back in.

On 10 similar nodes, this took 1 minute and 2 seconds. There is a point where it won't become any faster.

On 20 nodes(with \emph{mapred.map.tasks=160}), it takes 52 seconds (probably not worth the extra cost ... )

\resetcurrentobjects
\hypertarget{--doc-examples}{}

\chapter{Examples}


\section{\texttt{rhlapply}}


\subsection{Simple Example}

Take a sample of 100 iid observations Xi from N(0,1). Compute the mean of the eight closest neighbours to X1. This is repeated 1,000,000 times.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
nbrmean \PYG{o}{@textless[]-} \PYG{k+kr}{function}\PYG{p}{(}r\PYG{p}{)}\PYG{p}{\PYGZob{}}
  d \PYG{o}{@textless[]-} matrix\PYG{p}{(}rnorm\PYG{p}{(}\PYG{l+m}{200}\PYG{p}{)}\PYG{p}{,}ncol\PYG{o}{=}\PYG{l+m}{2}\PYG{p}{)}
  orig \PYG{o}{@textless[]-} d\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{,}\PYG{p}{]}
  ds \PYG{o}{@textless[]-} sort\PYG{p}{(}apply\PYG{p}{(}d\PYG{p}{,}\PYG{l+m}{1}\PYG{p}{,}\PYG{k+kr}{function}\PYG{p}{(}r\PYG{p}{)} sqrt\PYG{p}{(}sum\PYG{p}{(}\PYG{p}{(}r\PYG{o}{-}orig\PYG{p}{)}\PYG{o}{\PYGZca{}}\PYG{l+m}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{-1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{[}\PYG{l+m}{1}:\PYG{l+m}{8}\PYG{p}{]}
  mean\PYG{p}{(}ds\PYG{p}{)}
\PYG{p}{\PYGZcb{}}
trials \PYG{o}{@textless[]-} \PYG{l+m}{1000000}
\end{Verbatim}

\textbf{One Machine}

\code{trials} is 1,000,000

\begin{Verbatim}[commandchars=\\\{\}]
system.time\PYG{p}{(}\PYG{p}{\PYGZob{}}r \PYG{o}{@textless[]-} sapply\PYG{p}{(}\PYG{l+m}{1}:trials\PYG{p}{,} nbrmean\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
 user   system  elapsed
 \PYG{l+m}{1603.414}    \PYG{l+m}{0.127} \PYG{l+m}{1603.789}
\end{Verbatim}

\textbf{Distributed, output to file}

\begin{Verbatim}[commandchars=\\\{\}]
mapred \PYG{o}{@textless[]-} list\PYG{p}{(}mapred.map.tasks\PYG{o}{=}\PYG{l+m}{1000}\PYG{p}{)}
r \PYG{o}{@textless[]-} rhlapply\PYG{p}{(}\PYG{l+m}{1000000}\PYG{p}{,} fun\PYG{o}{=}nbrmean\PYG{p}{,}ofolder\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{/test/one"}\PYG{p}{,}mapred\PYG{o}{=}mapred\PYG{p}{)}
rhex\PYG{p}{(}r\PYG{p}{)}
\end{Verbatim}

Which took 7 minutes on a 4 core machine running 6 JVMs at once.


\subsection{Using Shared Files and Side Effects}

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
h\PYG{o}{=}rhlapply\PYG{p}{(}length\PYG{p}{(}simlist\PYG{p}{)}
  \PYG{p}{,}func\PYG{o}{=}\PYG{k+kr}{function}\PYG{p}{(}r\PYG{p}{)}\PYG{p}{\PYGZob{}}
    \PYG{c+c1}{@#@# do something from data loaded from session.Rdata}
    pdf\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{tmp/a.pdf"}\PYG{p}{)}
    plot\PYG{p}{(}animage\PYG{p}{)}
    dev.off\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
  setup\PYG{o}{=}expression\PYG{p}{(}\PYG{p}{\PYGZob{}}
    load\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{session.Rdata"}\PYG{p}{)}
  \PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{,}
  hadoop\PYG{o}{=}list\PYG{p}{(}mapred.map.tasks\PYG{o}{=}\PYG{l+m}{1000}\PYG{p}{)}\PYG{p}{,}
  shared\PYG{o}{=}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{/tmp/session.Rdata"}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{@#@#session.Rdata created by rhsave(..., file="/tmp/session.Rdata")}
\end{Verbatim}

Here \code{session.Rdata} is copied from HDFS to local temporary directories (making for faster reads). This
is a useful idiom for loading code that the \code{rhlapply} function might depend on. For example, assuming the image is not \emph{huge}

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
rhsave.image\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{/tmp/myimage.Rdata"}\PYG{p}{)}
rhlapply\PYG{p}{(}N\PYG{p}{,}\PYG{k+kr}{function}\PYG{p}{(}r\PYG{p}{)} \PYG{p}{\PYGZob{}}
  object \PYG{o}{@textless[]-} dataset\PYG{p}{[}\PYG{p}{[}r\PYG{p}{]}\PYG{p}{]}
  G\PYG{p}{(}object\PYG{p}{)}
\PYG{p}{\PYGZcb{}}\PYG{p}{,}setup\PYG{o}{=}expression\PYG{p}{(}\PYG{p}{\PYGZob{}}load\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{myimage.Rdata"}\PYG{p}{)}\PYG{p}{\PYGZcb{}}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

In the above example, I wish to apply the \code{G} to every element in \code{dataset}.


\section{\texttt{rhmr}}


\subsection{Word Count}

Generate the words, 1 word every line

\begin{Verbatim}[commandchars=\\\{\}]
rhlapply\PYG{p}{(}\PYG{l+m}{10000}\PYG{p}{,}\PYG{k+kr}{function}\PYG{p}{(}r\PYG{p}{)} paste\PYG{p}{(}sample\PYG{p}{(}letters\PYG{p}{[}\PYG{l+m}{1}:\PYG{l+m}{10}\PYG{p}{]}\PYG{p}{,}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{,}collapse\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{,}output.folder\PYG{o}{=}\PYG{l+s}{'}\PYG{l+s}{/tmp/words'}\PYG{p}{)}
\end{Verbatim}

Word count using the sequence file

Run it

\begin{Verbatim}[commandchars=\\\{\}]
z \PYG{o}{@textless[]-} rhmr\PYG{p}{(}map\PYG{o}{=}m\PYG{p}{,}reduce\PYG{o}{=}r\PYG{p}{,}inout\PYG{o}{=}c\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{sequence"}\PYG{p}{,}\PYG{l+s}{"}\PYG{l+s}{sequence"}\PYG{p}{)}\PYG{p}{,}
       ifolder\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{/tmp/words"}\PYG{p}{,}ofolder\PYG{o}{=}\PYG{l+s}{'}\PYG{l+s}{/tmp/wordcount'}\PYG{p}{)}
 rhex\PYG{p}{(}z\PYG{p}{)}
\end{Verbatim}


\subsection{Subset a file}

We can use this RHIPE to subset files. Setting \code{mapred.reduce.tasks} to 5 writes the subsetted data across 5 files (even though we haven't provided a reduce task)

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
m \PYG{o}{@textless[]-} expression\PYG{p}{(}\PYG{p}{\PYGZob{}}
  \PYG{k+kr}{for}\PYG{p}{(}x in map.values\PYG{p}{)}\PYG{p}{\PYGZob{}}
    y \PYG{o}{@textless[]-} strsplit\PYG{p}{(}x\PYG{p}{,}\PYG{l+s}{"}\PYG{l+s}{ +"}\PYG{p}{)}\PYG{p}{[}\PYG{p}{[}\PYG{l+m}{1}\PYG{p}{]}\PYG{p}{]}
    \PYG{k+kr}{for}\PYG{p}{(}w in y\PYG{p}{)} rhcollect\PYG{p}{(}w\PYG{p}{,}\PYG{k+kVariable}{T}\PYG{p}{)}
  \PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
z \PYG{o}{@textless[]-} rhmr\PYG{p}{(}map\PYG{o}{=}m\PYG{p}{,}inout\PYG{o}{=}c\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{text"}\PYG{p}{,}\PYG{l+s}{"}\PYG{l+s}{binary"}\PYG{p}{)}\PYG{p}{,}
    ifolder\PYG{o}{=}\PYG{l+s}{"}\PYG{l+s}{X"}\PYG{p}{,}ofolder\PYG{o}{=}\PYG{l+s}{'}\PYG{l+s}{Y'}\PYG{p}{,}mapred\PYG{o}{=}list\PYG{p}{(}mapred.reduce.tasks\PYG{o}{=}\PYG{l+m}{5}\PYG{p}{)}\PYG{p}{)}
rhex\PYG{p}{(}z\PYG{p}{)}
\end{Verbatim}

\resetcurrentobjects
\hypertarget{--doc-FAQ}{}

\chapter{FAQ}
\begin{enumerate}
\item {} 
Local Testing?

\end{enumerate}

Easily enough. In \code{rhmr} or \code{rhlapply}, set \code{mapred.job.tracker} to
`local' in the \code{mapred} option of the respective command. This will
use the local jobtracker to run your commands.

However keep in mind,
\code{shared.files} will not work, i.e those files will not be copied to the
working directory and side effect files will not be copied back.
\begin{enumerate}
\item {} 
Speed?

\end{enumerate}

Similar to Hadoop Streaming. The bottlenecks are writing and reading to STDIN
pipes and R.
\begin{enumerate}
\item {} 
What can RHIPE do?

\end{enumerate}

Firstly, there are several R packages for parrallel computing. \code{snow},{}`{}`snowfall{}`{}`
are packages for (mostly) embarrrasingly parallel computation and do not work
with massive datasets. \code{mapreduce} implements the mapreduce algorithm on a
single machine(which can be done with RHIPE by using a cluster of size 1).

RHIPE is a wrapper around Hadoop for the R user. So that he/she need not leave
the R environment for writing, running mapreduce applications and computing wth
massive datasets.
\begin{enumerate}
\item {} 
The command runner, different client and tasktrackers.

\end{enumerate}

The object passed to rhex has variable called \code{rhipe\_command} which is the
command of the program that Hadoop sends information to. In case the client
machine's (machine from which commands are being sent ) R installation is different from the
tasktrackers' R installation the RHIPE command runner wont be found. For example
suppose my cluster is linux and my client is OS X , then the \code{rhipe\_command}
variable will reflect the location of the rhipe command runner on OS X and not
that of the taskttrackers(Linux) R distribution.

There are two ways to fix this
a) after \code{z \textless{}- rhmr(...)} change \code{r{[}{[}1{]}{]}\$rhipe\_command} to the
value it should be on the tasktrackers.
(in case of \code{rhlapply}, it should be \code{r{[}{[}1{]}{]}{[}{[}1{]}{]}\$rhipe\_command})

or

b) set the environment variable \code{RHIPECOMMAND} on each of tasktrackers. RHIPE
java client will read this first before reading the above variable.
\begin{description}
\item[for x in spica deneb mimosa adhara castor acrux ;do]
echo -en `E{[}1;31m'
echo ``=========='' \$x ``==========''
tput sgr0
scp Rhipe\_0.52.tar.gz \$x:/tmp/
ssh \$x ``. \textasciitilde{}/.bashrc \&\& rm -rf /ln/meraki/custom/lib64/R/library/00LOCK \&\& R CMD INSTALL /tmp/Rhipe\_0.52.tar.gz''

\end{description}

done
\begin{enumerate}
\item {} 
Data types

\end{enumerate}

Stick to vectors of raws, character, logical, integer, complex and reals.  For
atomic vectors, don't use attributes (especially not the names attribute) \emph{Stay
away} from \code{data.frames} (These two(data.frames and named scalar vectors) are
read and written successfully, but I'm not guaranteeing success)

In lists, the names are preserved.

Try and keep your objects simple (using types even more basic than R types :) ) and even on data sets, you find no object corruption, there can be on large data sets  - ** if you use the advanced types such classes, data.frames etc **

6. Key and Value Object Size : Are there limits?
Yes, the serialized version of a key and object should be less than 64MB. I can fix this and will in future. For e.g. \code{runif(8e6)} is 61MB. Your keys and values should be less than this.
\begin{enumerate}
\item {} 
\code{java.lang.RuntimeException: RHMRMapRed.waitOutputThreads(): subprocess failed with code 141}

\end{enumerate}

This is because Hadoop broke the read/write pipe with the R code. To view the error, you'll need to go the job tracker website, click on one of the Failed attempts and see the error.

\resetcurrentobjects
\hypertarget{--doc-ProtoBuffers}{}

\chapter{Protobuffer and R}

A package called rprotobuf which implements a simple serialization using Googles
protocol buffers{[}1{]}.  The package also includes some miscellaneous functions for
writing/reading variable length encoded integers, and Base64 encoding/decoding
related functions.  The package can be downloaded from
\href{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz}{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz} . It requires one to install \code{libproto}
(Googles protobuffer library)

\textbf{Requirements}

Google's Protocol Buffer library. See {[}1{]}.

\textbf{Brief Description}

The R objects that can be serialized are numerics,complex,integers,strings, logicals,
raw,nulls and lists.  Attributes of the aforementioned are preserved. NA is also
preserved(for the above) As such, the objects include factors and matrices.  The proto file can be
found in the source.

Serialization/deserialization works perfectly for these types.

\textbf{Extras}

With version 1.1, \code{rprotobuf} will now serialize
\begin{itemize}
\item {} 
SYMSXP,

\item {} 
LISTSXP

\item {} 
CLOSXP

\item {} 
ENVSXP ( no locking )

\item {} 
PROMSXP

\item {} 
LANGSXP

\item {} 
DOTSXP

\item {} 
S4SXP

\item {} 
EXPRSXP

\end{itemize}

Serialization/deserialization(for these extras SEXP types)  \emph{appear} to work but I cannot prove that (one with a thorough knowledge of R internals needs to audit the code( \code{src/message.cc} )). They remain undocumented in the help pages.

\textbf{Regrets}

\code{serialize.c} (in R 2.9 sources) uses a hashtable to add references to previously added environments and symbols(instead of adding them again). This reduces the size of the serialized expresson. \code{rprotobuf} does not do any such thing. It ought to and in future it will.

\textbf{Download}

Package(with source) : \href{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz}{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz}

Install

\begin{Verbatim}[commandchars=\\\{\}]
R CMD INSTALL rprotobuf\PYGZus{}1.1.tar.gz
\end{Verbatim}

{[}1{]} \href{http://code.google.com/apis/protocolbuffers/docs/overview.html}{http://code.google.com/apis/protocolbuffers/docs/overview.html}

\resetcurrentobjects
\hypertarget{--doc-datatypes}{}

\chapter{Datatypes}

I have tried to make RHIPE as flexible as possible with regards to data types exchanged. As such the following can be sent
\begin{itemize}
\item {} 
atomic vectors (raw, character, logical, complex, real, integer) (these include NA's)

\item {} 
lists of the above and lists of lists. Names will be preserved.

\end{itemize}

So \emph{officially} no matrices, data.frames, time series objects etc. If there are needed , serialize them using \code{serialize} .

\emph{Unofficially}, several attributes of are serialized, hence you can send matrices (the dim and  dimnames attribute are involved) will be serialized.
However, with regards to data.frames the row.names attribute has caused me much grief. Keep it simple: scalar vectors (use \code{as.vector} to remove attributes) and lists (names are allowed).

I use data.frames too and it works. If you get a crash(error code 139,using version \textgreater{}= 0.53), email me.

If in doubt, serialize.


\renewcommand{\indexname}{Module Index}
\printmodindex
\renewcommand{\indexname}{Index}
\printindex
\end{document}
