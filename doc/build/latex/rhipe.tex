% Generated by Sphinx.
\documentclass[letterpaper,10pt,english]{manual}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}


\title{rhipe Documentation}
\date{December 04, 2009}
\release{0.51}
\author{Saptarshi Guha}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex
\makemodindex
\newcommand\PYGZat{@}
\newcommand\PYGZlb{[}
\newcommand\PYGZrb{]}
\newcommand\PYGaz[1]{\textcolor[rgb]{0.00,0.63,0.00}{#1}}
\newcommand\PYGax[1]{\textcolor[rgb]{0.84,0.33,0.22}{\textbf{#1}}}
\newcommand\PYGay[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGar[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGas[1]{\textcolor[rgb]{0.25,0.44,0.63}{\textit{#1}}}
\newcommand\PYGap[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaq[1]{\textcolor[rgb]{0.38,0.68,0.84}{#1}}
\newcommand\PYGav[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaw[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGat[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGau[1]{\textcolor[rgb]{0.32,0.47,0.09}{#1}}
\newcommand\PYGaj[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGak[1]{\textcolor[rgb]{0.14,0.33,0.53}{#1}}
\newcommand\PYGah[1]{\textcolor[rgb]{0.00,0.13,0.44}{\textbf{#1}}}
\newcommand\PYGai[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGan[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGao[1]{\textcolor[rgb]{0.25,0.44,0.63}{\textbf{#1}}}
\newcommand\PYGal[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGam[1]{\textbf{#1}}
\newcommand\PYGab[1]{\textit{#1}}
\newcommand\PYGac[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaa[1]{\textcolor[rgb]{0.19,0.19,0.19}{#1}}
\newcommand\PYGaf[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGag[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGad[1]{\textcolor[rgb]{0.00,0.25,0.82}{#1}}
\newcommand\PYGae[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaZ[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGbf[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaX[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaY[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGbc[1]{\textcolor[rgb]{0.78,0.36,0.04}{#1}}
\newcommand\PYGbb[1]{\textcolor[rgb]{0.00,0.00,0.50}{\textbf{#1}}}
\newcommand\PYGba[1]{\textcolor[rgb]{0.02,0.16,0.45}{\textbf{#1}}}
\newcommand\PYGaR[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaS[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaP[1]{\textcolor[rgb]{0.05,0.52,0.71}{\textbf{#1}}}
\newcommand\PYGaQ[1]{\textcolor[rgb]{0.78,0.36,0.04}{\textbf{#1}}}
\newcommand\PYGaV[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGaW[1]{\textcolor[rgb]{0.05,0.52,0.71}{\textbf{#1}}}
\newcommand\PYGaT[1]{\textcolor[rgb]{0.73,0.38,0.84}{#1}}
\newcommand\PYGaU[1]{\textcolor[rgb]{0.13,0.50,0.31}{#1}}
\newcommand\PYGaJ[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand\PYGaK[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaH[1]{\textcolor[rgb]{0.50,0.00,0.50}{\textbf{#1}}}
\newcommand\PYGaI[1]{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{#1}}
\newcommand\PYGaN[1]{\textcolor[rgb]{0.73,0.73,0.73}{#1}}
\newcommand\PYGaO[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaL[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand\PYGaM[1]{\colorbox[rgb]{1.00,0.94,0.94}{\textcolor[rgb]{0.25,0.50,0.56}{#1}}}
\newcommand\PYGaB[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGaC[1]{\textcolor[rgb]{0.33,0.33,0.33}{\textbf{#1}}}
\newcommand\PYGaA[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand\PYGaF[1]{\textcolor[rgb]{0.63,0.00,0.00}{#1}}
\newcommand\PYGaG[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand\PYGaD[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand\PYGaE[1]{\textcolor[rgb]{0.25,0.50,0.56}{\textit{#1}}}
\newcommand\PYGbg[1]{\textcolor[rgb]{0.44,0.63,0.82}{\textit{#1}}}
\newcommand\PYGbe[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand\PYGbd[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand\PYGbh[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\begin{document}

\maketitle
\tableofcontents



Mainpage

\resetcurrentobjects
\hypertarget{--doc-installation}{}

\chapter{Setting up RHIPE}


\section{Requirements}
\begin{enumerate}
\item {} 
\emph{Protobuffers}

RHIPE uses Google's Protobuf library for serialization. This(the C/C++
libraries) must be installed on \emph{all} machines (master/workers). Get
Protobuffers from \href{http://code.google.com/p/protobuf/}{http://code.google.com/p/protobuf/}. RHIPE already has the
protobuf jar file inside it.
\begin{description}
\item[Non Standard Locations]
If installing protobuf to a non standard location, update the
PKG\_CONFIG\_PATH variable, e.g

\begin{Verbatim}[commandchars=@\[\]]
export PKG@_CONFIG@_PATH@PYGbe[=]@$PKG@_CONFIG@_PATH:@$CUSTROOT@PYGbe[/]lib@PYGbe[/]pkgconfig@PYGbe[/]
\end{Verbatim}

\end{description}

\item {} 
\emph{R} , tested on 2.8

\item {} 
\emph{rJava} The R package needs rJava.

\end{enumerate}

Tested on RHEL Linux, Mac OS 10.5.5 (Leopard).
Does not work on Snow Leopard


\section{Installation}

Rhipe requires the following environment variables

\begin{Verbatim}[commandchars=@\[\]]
HADOOP@PYGbe[=]location of Hadoop installation
HADOOP@_LIB@PYGbe[=]location of lib jar files included with Hadoop, usually
@$HADOOP@PYGbe[/]lib
HADOOP@_CONF@_DIR@PYGbe[=]location of Hadoop conf folder, (usually @$HADOOP@PYGbe[/]conf)
\end{Verbatim}

On every machine

\begin{Verbatim}[commandchars=@\[\]]
R CMD INSTALL Rhipe@_VERSION.tar.gz
\end{Verbatim}

To load it

\begin{Verbatim}[commandchars=@\[\]]
library(Rhipe)
\end{Verbatim}

\resetcurrentobjects
\hypertarget{--doc-rhlapply}{}

\chapter{The \texttt{rhlapply} Command}


\section{Introduction}

\code{rhapply} applies a user defined function to the elements of a given
R list or the function can be run over the set of numbers from 1 to
n. In the former case the list is written to a sequence file,whose length is the
default setting of \code{rhwrite}.

Running a hundreds of thousadands of seperate trials
can be terribly inefficient, instead consider grouping them, i.e set
\code{mapred.max.tasks} to a value much smaller than the length of the
list.


\section{Return Value}

\code{rhlapply} returns a list, the names of which is equal to the names
of the input list (if given).


\section{Function Usage}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rhlapply @PYGbe[@textless[]-] @PYGap[function]( ll@PYGbe[=]@PYGaD[NULL],
                     fun,
                     ifolder@PYGbe[=]@PYGaB["]@PYGaB["],
                     ofolder@PYGbe[=]@PYGaB["]@PYGaB["],
                     readIn@PYGbe[=]T,
                     inout@PYGbe[=]c('lapply','sequence')
                     mapred@PYGbe[=]list()
                     setup@PYGbe[=]@PYGaD[NULL],jobname@PYGbe[=]@PYGaB["]@PYGaB[rhlapply"],doLocal@PYGbe[=]F,...
                     )
\end{Verbatim}

Description follows
\begin{description}
\item[\code{ll}]
The list object, optional. Applies \code{fun} to \code{ll{[}{[}i{]}{]}} .
If instead \code{ll} is a numeric, applies \code{fun} to each element of
\code{seq(1,ll)}. If not given, must provide a value for \code{ifolder}

\item[\code{fun}]
A function that takes only one argument.

\item[\code{ifolder}]
If \code{ll} is null, provide a source here. Also change the value of
\code{inout{[}1{]}} to either \code{text} or \code{sequence}.

\item[\code{readIn}]
The results are stored in a temporary sequence file on the DFS which is
deleted. Should the results be returned in a list? Default is TRUE. For
large number of output key-values (e.g 1MM) set this to FALSE, using the
default options to \code{rhread} is extremely slow.

\item[\code{ofolder}]
If given the results are written to this folder and not deleted. If not,
they are written to temporary folder, read back in (assuming \code{readIn}
is TRUE) and deleted.

\item[\code{mapred}]
Options passed onto \code{rhmr}

\item[\code{setup}]
And expression that is called before running \code{func}. Called once per
JVM.

\item[\code{doLocal}]
Default is \code{F}. Sent to \code{rhread}

\item[\code{...}]
passed onto RHMR.

\end{description}


\subsection{RETURN}

An object that is passed onto \code{rhex}.


\subsection{IMPORTANT}

The object passed to rhex has variable called \code{rhipe\_command} which is the
command of the program that Hadoop sends information to. In case the client
machine's (machine from which commands are being sent ) R installation is different from the
tasktrackers' R installation the RHIPE command runner wont be found. For example
suppose my cluster is linux and my client is OS X , then the \code{rhipe\_command}
variable will reflect the location of the rhipe command runner on OS X and not
that of the taskttrackers(Linux) R distribution.

There are two ways to fix this
a) after \code{z \textless{}- rhlapply(...)} change \code{r{[}{[}1{]}{]}{[}{[}1{]}{]}\$rhipe\_command} to the
value it should be on the tasktrackers.

or

b) set the environment variable \code{RHIPECOMMAND} on each of tasktrackers. RHIPE
java client will read this first before reading the above variable.

\resetcurrentobjects
\hypertarget{--doc-rhmr}{}

\chapter{The \texttt{rhmr} Command}


\section{Introduction}

The \code{rhmr} command runs a general mapreduce program using user supplied map
and reduce commands.


\section{Return Value}

In general a set of files on the Hadoop Distributed File System. It can be of
Text Format or a Sequence file format. In case of the latter, the key and values
can be any R data structure.


\section{Function}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rhmr @PYGbe[@textless[]-] @PYGap[function](map,reduce@PYGbe[=]@PYGaD[NULL],
         combiner@PYGbe[=]F, @PYGaf[@#CANNOT BE CHANGED]
         setup@PYGbe[=]@PYGaD[NULL],
         cleanup@PYGbe[=]@PYGaD[NULL],
         ofolder@PYGbe[=]'',
         ifolder@PYGbe[=]'',
         inout@PYGbe[=]c(@PYGaB["]@PYGaB[text"],@PYGaB["]@PYGaB[text"]),
         mapred@PYGbe[=]@PYGaD[NULL],
         shared@PYGbe[=]c(),
         jarfiles@PYGbe[=]c(),
         copyFiles@PYGbe[=]F,
         opts@PYGbe[=]rhoptions(),jobname@PYGbe[=]@PYGaB["]@PYGaB["])
\end{Verbatim}
\begin{description}
\item[\code{map}]
A map expression, not a function. The map expression can expect a list of keys in \code{map.keys} and list of values in \code{map.values}.

\item[\code{reduce}]
Can be null if only a map job. If not,reduce should be an expression with three attributes
\begin{description}
\item[\code{pre}]
Called for a new key, but no values have been read. The key is present in \code{reduce.key}.

\item[\code{reduce}]
Called for reducing the incoming values. The values are in a list called \code{reduce.values}

\item[\code{post}]
Called when all the values have been sent.

\end{description}

\item[\code{combiner}]
Uses a combiner if TRUE. If so, then \code{reduce.values} present in the \code{reduce\$reduce} expression will be a \emph{subset} of values.

\item[\code{setup}]
An expression that can be called to setup the environment. Called once for every task.
It can be a list of two attributes \code{map} and \code{reduce} which are expressions to be run in the map and reduce stage. If a single expression then that is run for both map and reduce

\item[\code{cleanup}]
Same as for \code{setup}, run when all work for a task is complete.

\item[\code{ifolder}]
A folder or file to be processed. Can be a vector of strings.

\item[\code{ofolder}]
The folder to store output in. Side effects will be copied here.

\item[\code{inout{}`}]\begin{description}
\item[A vector of input type and output type.]\begin{description}
\item[\code{text}]
indicates Text Format. Use \code{mapred.field.separator} to seperate the elements of a vector.

\end{description}

\item[\code{sequence}]
is a sequence format. Outputs in this form /can/ be used as an input.

\item[\code{binary}]
is a simple binary format consisting of key-length, key data, value-length, value data where the lengths are integers in network order. Though \emph{much} faster than sequence in terms of reading in data, it \emph{cannot} be used an input to a map reduce operation.

\end{description}

\item[\code{shared}]
A vector of files on the HDFS that will be copied to the working directory of the R program. These files can then be loaded as easily as \code{load(filename)} (removed leading path)

\item[\code{jarfiles}]
Copy jar files if required. Experimental, probably doesn't work.

\item[\code{copyFiles}]
For side effects to be copied back to the DFS, set this to TRUE, otherwise they wont be copied.

\item[\code{mapred}]
Set Hadoop options here and RHIPE options.

\item[\code{jobname}]
the jobname, if not given, then current date and time is the job title.

\end{description}


\section{RHIPE Options}
\begin{description}
\item[\textbf{rhipe\_stream\_buffer}]
The size of the STDIN buffer used to write data to the R process(in bytes)
\emph{default:} 10*1024 bytes

\item[\textbf{mapred.textoutputformat.separator}]
The text that seperates the key from value when \code{inout{[}2{]}} equals text.
\emph{default:} Tab

\item[\textbf{mapred.field.separator}]
The text that seperates fields when \code{inout{[}2{]}} equals text.
\emph{default:} Space

\item[\textbf{rhipe\_reduce\_buff\_size}]
The maximum length of \code{reduce.values}
\emph{default:} 10,000

\item[\textbf{rhipe\_map\_buff\_size}]
The maximum length of \code{map.values} (and \code{map.keys})
\emph{default:} 10,000

\end{description}


\section{Status, Counters and Writing Output}


\subsection{Status}

To update the status use \code{rhstatus} which takes a single string e.g \code{rhstatus("Nice")}
This will also indicate progress.


\subsection{Counter}

To update the counter C in the group G with a number N, user \code{rhcounter(G,C,N)}
where C and G are strings and N is a number.


\subsection{Output}

To output data use \code{rhcollect(KEY,VALUE)} where KEY and VALUE are R objects that can be serialized by \code{rhsz} (see the misc page). If one needs to send across complex R objects e.g the KEY is a function, do something like \code{rhcollect(serialize(KEY,NULL),VALUE)}


\section{Side Effect files}

Files written to \code{tmp/} (no leading slash !) e.g \code{pdf("tmp/x.pdf")} will be copied to the output folder.


\section{Mapreduce Options}

Many mapreduce configuration variables are stored in the environment. To get the value use \code{Sys.getenv()}, e.g in the map stage, to find out the name of the current input file, use \code{Sys.getenv('mapred.input.file')} .


\section{IMPORTANT}

The object passed to rhex has variable called \code{rhipe\_command} which is the
command of the program that Hadoop sends information to. In case the client
machine's (machine from which commands are being sent ) R installation is different from the
tasktrackers' R installation the RHIPE command runner wont be found. For example
suppose my cluster is linux and my client is OS X , then the \code{rhipe\_command}
variable will reflect the location of the rhipe command runner on OS X and not
that of the taskttrackers(Linux) R distribution.

There are two ways to fix this
a) after \code{z \textless{}- rhmr(...)} change \code{r{[}{[}1{]}{]}\$rhipe\_command} to the
value it should be on the tasktrackers.

or

b) set the environment variable \code{RHIPECOMMAND} on each of tasktrackers. RHIPE
java client will read this first before reading the above variable.

\resetcurrentobjects
\hypertarget{--doc-rhmisc}{}

\chapter{Miscellaneous Commands}


\section{Introduction}

This is a list of supporting functions for reading, writing sequence files and
manipulating files on the Hadoop Distributed File System (HDFS).


\section{Serialization}


\subsection{rhsz}

\begin{Verbatim}[commandchars=@\[\]]
rhsz @PYGbe[@textless[]-] @PYGap[function](object)
\end{Verbatim}

Serializes a given R object. Currently the only objects that can be serialized
are vectors of Raws,Numerics, Integers, Strings(including NA), Logical(including NA)
and lists of these and lists of lists of these. Attributes are copied to(e.g
names attributes). It appears objects like matrices, factors also get serialized
and unserialized sucessfully.


\subsection{rhuz}

\begin{Verbatim}[commandchars=@\[\]]
rhuz @PYGbe[@textless[]-] @PYGap[function](object)
\end{Verbatim}

Unserializes a raw object returned from \code{rhsz}


\section{HDFS Related}


\subsection{rhsave}

\begin{Verbatim}[commandchars=@\[\]]
rhsave @PYGbe[@textless[]-] @PYGap[function](..., file)
\end{Verbatim}

Saves the objects in \code{...} to \code{file} on the HDFS. All other options are
passed onto the R function \code{save}


\subsection{rhsave.image}

\begin{Verbatim}[commandchars=@\[\]]
rhsave.image @PYGbe[@textless[]-] @PYGap[function](..., file)
\end{Verbatim}

Same as R's \code{save.image}, except that the file goes to the HDFS.


\subsection{rhput}

\begin{Verbatim}[commandchars=@\[\]]
rhput @PYGbe[@textless[]-] @PYGap[function](src,dest,deleteDest@PYGbe[=]@PYGaD[TRUE])
\end{Verbatim}

Copies the file in \code{src} to the \code{dest} on the HDFS, deleting destination if
\code{deleteDest} is TRUE.


\subsection{rhget}

\begin{Verbatim}[commandchars=@\[\]]
rhget @PYGbe[@textless[]-] @PYGap[function](src,dest)
\end{Verbatim}

Copies \code{src{}`{}`(on the HDFS) to {}`{}`dest} on the local. If \code{src} is a directory and \code{dest} exists,
\code{src} is copied inside \code{dest{}`{}`(i.e a folder inside {}`{}`dest}).If not(i.e
\code{dest} does not exist), \code{src}`s contents is copied to a new folder called
\code{dest}.  If \code{src} is a file, and \code{dest} is a directory \code{src} is copied
inside \code{dest} . If \code{dest} does not exist, it is copied to that file

Wildcards allowed

OVERWRITES!


\subsection{rhls}

\begin{Verbatim}[commandchars=@\[\]]
rhls @PYGbe[@textless[]-] @PYGap[function](dir)
\end{Verbatim}

Lists the path at \code{dir}. Wildcards allowed.


\subsection{rhdel}

\begin{Verbatim}[commandchars=@\[\]]
rhdel @PYGbe[@textless[]-] @PYGap[function](dir)
\end{Verbatim}

Deletes file(s) at/in \code{dir}. Wildcards allowed.


\subsection{rhwrite}

\begin{Verbatim}[commandchars=@\[\]]
rhwrite @PYGbe[@textless[]-] @PYGap[function](lo,f,n@PYGbe[=]@PYGaD[NULL],...)
\end{Verbatim}

Writes the list \code{lo}  to the file \code{f}. \code{n} is the number of sequence files
to split the list into.  The default value of \code{n} is
\code{mapred.map.tasks} * \code{mapred.tasktracker.map.tasks.maximum} .


\subsection{rhread}

\begin{Verbatim}[commandchars=@\[\]]
rhread @PYGbe[@textless[]-] @PYGap[function](files,verbose@PYGbe[=]T,doLocal@PYGbe[=]T)
\end{Verbatim}

Reads files(s) from \code{files} (which could be a directory). Wildcards allowed.

If \code{verbose} is True, information is displayed (useful when reading many
files)
\code{rhread} read sequence files by running a mapreduce   job to convert the
sequence file to a binary file.
This is then merged and read into R. If \code{doLocal} is True this mapreduce
conversion job is a local mapreduce job (which can be slow for lots of part
files) else a fully distributed job.


\subsection{rhmerge}

\begin{Verbatim}[commandchars=@\[\]]
rhmerge(inr,ou)
\end{Verbatim}

\code{inr} can have wildcards. Usually used to merge all files in a directory into one file \code{ou} on the local file system.


\subsection{rhreadBin}

\begin{Verbatim}[commandchars=@\[\]]
rhreadBin @PYGbe[@textless[]-] @PYGap[function](filename, max@PYGbe[=]as.integer(@PYGaS[-1]), bf@PYGbe[=]as.integer(@PYGaS[0]))
\end{Verbatim}

Reads data outputed in `binary' form. \code{max} is the maximum number to read, -1
is all. \code{bf} is the read buffer, 0 implies the os specified default \code{BUFSIZ}

\resetcurrentobjects
\hypertarget{--doc-ec2}{}

\chapter{Using RHIPE on EC2}


\section{Introduction}

There is one 32 bit EC2 AMI with R-2.8, Hadoop 0.21 and the latest RHIPE. \href{http://s3sync.net/wiki}{s3sync} is also present.

The following describes the usage of the EC2 scripts.


\section{Usage}
\begin{itemize}
\item {} 
Get an Amazon EC2 account and confirm the ability to start and instance from the command line (using ec2-tools).

\item {} 
Unzip the rhipe-ec2 distribution (see the downloads page)

\item {} 
OPTIONS

\end{itemize}

In \code{bin/hadoop-ec2-env.sh} template there are several options:
\begin{description}
\item[AWS\_ACCOUNT\_ID]
fill this from the Amazon Account Identifiers

\item[AWS\_ACCESS\_KEY\_ID]
same as above

\item[AWS\_SECRET\_ACCESS\_KEY]
same as above

\item[R\_USER\_FILE]
a URL to an R script. This file is executed on machine boot up. Useful to install R packages. Read \code{bin/hadoop-ec2-env.sh.template} for details.

\item[INSTANCE\_TYPE]
choose the Amazon machine instance type. For details, go to
\href{http://aws.amazon.com/ec2/instance-types/}{http://aws.amazon.com/ec2/instance-types/}

\end{description}
\begin{itemize}
\item {} 
Save the file as \code{bin/hadoop-ec2-env.sh}

\end{itemize}


\section{Some launch commands}
\begin{itemize}
\item {} 
launch

\end{itemize}

\begin{Verbatim}[commandchars=@\[\]]
bin/hadoop-ec2 launch-cluster clustername number-of-workers
\end{Verbatim}

Replace clustername with the name of the cluster and number-of-workers with the number of workers. Use Elasticfox to check all the instances are running, this can some time.
\begin{itemize}
\item {} 
login

\end{itemize}

\begin{Verbatim}[commandchars=@\[\]]
bin/hadoop-ec2 login clustername
\end{Verbatim}
\begin{itemize}
\item {} 
terminate

\end{itemize}

\begin{Verbatim}[commandchars=@\[\]]
bin/hadoop-ec2 terminate-cluster clustername
\end{Verbatim}
\begin{itemize}
\item {} 
You can check the status of jobs at masterip:50030 in your web browser.

\end{itemize}


\section{Useful tools}
\begin{description}
\item[\href{http://www.s3fox.net/}{s3fox}]
A S3 file browser that works within Firefox.

\item[\href{http://sourceforge.net/projects/elasticfox/}{Elasticfox}]
EC2 management tools, a Firefox add-on.

\end{description}

\resetcurrentobjects
\hypertarget{--doc-examples}{}

\chapter{Examples}


\section{\texttt{rhlapply}}


\subsection{Simple Example}

Take a sample of 100 iid observations Xi from N(0,1). Compute the mean of the eight closest neighbours to X1. This is repeated 1,000,000 times.

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
nbrmean @PYGbe[@textless[]-] @PYGap[function](r){
  d @PYGbe[@textless[]-] matrix(rnorm(@PYGaS[200]),ncol@PYGbe[=]@PYGaS[2])
  orig @PYGbe[@textless[]-] d@PYGZlb[]@PYGaS[1],@PYGZrb[]
  ds @PYGbe[@textless[]-] sort(apply(d,@PYGaS[1],@PYGap[function](r) sqrt(sum((r@PYGbe[-]orig)@PYGbe[@textasciicircum[]]@PYGaS[2])))@PYGZlb[]@PYGaS[-1]@PYGZrb[])@PYGZlb[]@PYGaS[1]:@PYGaS[8]@PYGZrb[]
  mean(ds)
}
trials @PYGbe[@textless[]-] @PYGaS[1000000]
\end{Verbatim}

\textbf{One Machine}

\code{trials} is 1,000,000

\begin{Verbatim}[commandchars=@\[\]]
system.time({r @PYGbe[@textless[]-] sapply(@PYGaS[1]:trials, nbrmean)})
 user   system  elapsed
 @PYGaS[1603.414]    @PYGaS[0.127] @PYGaS[1603.789]
\end{Verbatim}

\textbf{Distributed, output to file}

\begin{Verbatim}[commandchars=@\[\]]
mapred @PYGbe[@textless[]-] list(mapred.map.tasks@PYGbe[=]@PYGaS[1000])
r @PYGbe[@textless[]-] rhlapply(@PYGaS[1000000], fun@PYGbe[=]nbrmean,ofolder@PYGbe[=]@PYGaB["]@PYGaB[/test/one"],mapred@PYGbe[=]mapred)
rhex(r)
\end{Verbatim}

Which took 7 minutes on a 4 core machine running 6 JVMs at once.


\subsection{Using Shared Files and Side Effects}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
h@PYGbe[=]rhlapply(length(simlist)
  ,func@PYGbe[=]@PYGap[function](r){
    @PYGaf[@#@# do something from data loaded from session.Rdata]
    pdf(@PYGaB["]@PYGaB[tmp/a.pdf"])
    plot(animage)
    dev.off()},
  setup@PYGbe[=]expression({
    load(@PYGaB["]@PYGaB[session.Rdata"])
  }),
  hadoop@PYGbe[=]list(mapred.map.tasks@PYGbe[=]@PYGaS[1000]),
  shared.files@PYGbe[=](@PYGaB["]@PYGaB[/tmp/session.Rdata"]))
\end{Verbatim}

Here \code{session.Rdata} is copied from HDFS to local temporary directories (making for faster reads). This
is a useful idiom for loading code that the \code{rhlapply} function might depend on. For example, assuming the image is not \emph{huge}

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
rhsave.image(@PYGaB["]@PYGaB[/tmp/myimage.Rdata"])
rhlapply(N,@PYGap[function](r) {
  object @PYGbe[@textless[]-] dataset@PYGZlb[]@PYGZlb[]r@PYGZrb[]@PYGZrb[]
  G(object)
},setup@PYGbe[=]expression({load(@PYGaB["]@PYGaB[myimage.Rdata"])}))
\end{Verbatim}

In the above example, I wish to apply the \code{G} to every element in \code{dataset}.


\section{\texttt{rhmr}}


\subsection{Word Count}

Generate the words, 1 word every line

\begin{Verbatim}[commandchars=@\[\]]
rhlapply(@PYGaS[10000],@PYGap[function](r) paste(sample(letters@PYGZlb[]@PYGaS[1]:@PYGaS[10]@PYGZrb[],@PYGaS[5]),collapse@PYGbe[=]@PYGaB["]@PYGaB["]),output.folder@PYGbe[=]'@PYGbe[/]tmp@PYGbe[/]words')
\end{Verbatim}

Word count using the sequence file

Run it

\begin{Verbatim}[commandchars=@\[\]]
z @PYGbe[@textless[]-] rhmr(map@PYGbe[=]m,reduce@PYGbe[=]r,inout@PYGbe[=]c(@PYGaB["]@PYGaB[sequence"],@PYGaB["]@PYGaB[sequence"]),
       ifolder@PYGbe[=]@PYGaB["]@PYGaB[/tmp/words"],ofolder@PYGbe[=]'@PYGbe[/]tmp@PYGbe[/]wordcount')
 rhex(z)
\end{Verbatim}


\subsection{Subset a file}

We can use this RHIPE to subset files. Setting \code{mapred.reduce.tasks} to 5 writes the subsetted data across 5 files (even though we haven't provided a reduce task)

\begin{Verbatim}[commandchars=@\[\],numbers=left,firstnumber=1,stepnumber=1]
m @PYGbe[@textless[]-] expression({
  @PYGap[for](x in map.values){
    y @PYGbe[@textless[]-] strsplit(x,@PYGaB["]@PYGaB[ +"])@PYGZlb[]@PYGZlb[]@PYGaS[1]@PYGZrb[]@PYGZrb[]
    @PYGap[for](w in y) rhcollect(w,T)
  }})
z @PYGbe[@textless[]-] rhmr(map@PYGbe[=]m,inout@PYGbe[=]c(@PYGaB["]@PYGaB[text"],@PYGaB["]@PYGaB[binary"]),
    ifolder@PYGbe[=]@PYGaB["]@PYGaB[X"],ofolder@PYGbe[=]'Y',mapred@PYGbe[=]list(mapred.reduce.tasks@PYGbe[=]@PYGaS[5]))
rhex(z)
\end{Verbatim}

\resetcurrentobjects
\hypertarget{--doc-FAQ}{}

\chapter{FAQ}
\begin{enumerate}
\item {} 
Local Testing?

\end{enumerate}

Easily enough. In \code{rhmr} or \code{rhlapply}, set \code{mapred.job.tracker} to
`local' in the \code{mapred} option of the respective command. This will
use the local jobtracker to run your commands.

However keep in mind,
\code{shared.files} will not work, i.e those files will not be copied to the
working directory and side effect files will not be copied back.
\begin{enumerate}
\item {} 
Speed?

\end{enumerate}

Similar to Hadoop Streaming. The bottlenecks are writing and reading to STDIN
pipes and R.
\begin{enumerate}
\item {} 
What can RHIPE do?

\end{enumerate}

Firstly, there are several R packages for parrallel computing. \code{snow},{}`{}`snowfall{}`{}`
are packages for (mostly) embarrrasingly parallel computation and do not work
with massive datasets. \code{mapreduce} implements the mapreduce algorithm on a
single machine(which can be done with RHIPE by using a cluster of size 1).

RHIPE is a wrapper around Hadoop for the R user. So that he/she need not leave
the R environment for writing, running mapreduce applications and computing wth
massive datasets.
\begin{enumerate}
\item {} 
The command runner, different client and tasktrackers.

\end{enumerate}

The object passed to rhex has variable called \code{rhipe\_command} which is the
command of the program that Hadoop sends information to. In case the client
machine's (machine from which commands are being sent ) R installation is different from the
tasktrackers' R installation the RHIPE command runner wont be found. For example
suppose my cluster is linux and my client is OS X , then the \code{rhipe\_command}
variable will reflect the location of the rhipe command runner on OS X and not
that of the taskttrackers(Linux) R distribution.

There are two ways to fix this
a) after \code{z \textless{}- rhmr(...)} change \code{r{[}{[}1{]}{]}\$rhipe\_command} to the
value it should be on the tasktrackers.
(in case of \code{rhlapply}, it should be \code{r{[}{[}1{]}{]}{[}{[}1{]}{]}\$rhipe\_command})

or

b) set the environment variable \code{RHIPECOMMAND} on each of tasktrackers. RHIPE
java client will read this first before reading the above variable.
\begin{enumerate}
\item {} 
Data types

\end{enumerate}

Stick to vectors of raws, character, logical, integer, complex and reals.
For atomic vectors, don't use attributes (especially not the names attribute)
\emph{Stay away} from \code{data.frames}

In lists, the names are preserved.

Try and keep your objects simple (using types even more basic than R types :) ) and even on data sets, you find no object corruption, there can be on large data sets  - ** if you use the advanced types such classes, data.frames etc **

\resetcurrentobjects
\hypertarget{--doc-ProtoBuffers}{}

\chapter{Protobuffer and R}

A package called rprotobuf which implements a simple serialization using Googles
protocol buffers{[}1{]}.  The package also includes some miscellaneous functions for
writing/reading variable length encoded integers, and Base64 encoding/decoding
related functions.  The package can be downloaded from
\href{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz}{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz} . It requires one to install \code{libproto}
(Googles protobuffer library)

\textbf{Requirements}

Google's Protocol Buffer library. See {[}1{]}.

\textbf{Brief Description}

The R objects that can be serialized are numerics,complex,integers,strings, logicals,
raw,nulls and lists.  Attributes of the aforementioned are preserved. NA is also
preserved(for the above) As such, the objects include factors and matrices.  The proto file can be
found in the source.

Serialization/deserialization works perfectly for these types.

\textbf{Extras}

With version 1.1, \code{rprotobuf} will now serialize
\begin{itemize}
\item {} 
SYMSXP,

\item {} 
LISTSXP

\item {} 
CLOSXP

\item {} 
ENVSXP ( no locking )

\item {} 
PROMSXP

\item {} 
LANGSXP

\item {} 
DOTSXP

\item {} 
S4SXP

\item {} 
EXPRSXP

\end{itemize}

Serialization/deserialization(for these extras SEXP types)  \emph{appear} to work but I cannot prove that (one with a thorough knowledge of R internals needs to audit the code( \code{src/message.cc} )). They remain undocumented in the help pages.

\textbf{Regrets}

\code{serialize.c} (in R 2.9 sources) uses a hashtable to add references to previously added environments and symbols(instead of adding them again). This reduces the size of the serialized expresson. \code{rprotobuf} does not do any such thing. It ought to and in future it will.

\textbf{Download}

Package(with source) : \href{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz}{http://ml.stat.purdue.edu/rpackages/rprotobuf\_1.1.tar.gz}

Install

\begin{Verbatim}[commandchars=@\[\]]
R CMD INSTALL rprotobuf@_1.1.tar.gz
\end{Verbatim}

{[}1{]} \href{http://code.google.com/apis/protocolbuffers/docs/overview.html}{http://code.google.com/apis/protocolbuffers/docs/overview.html}

\resetcurrentobjects
\hypertarget{--doc-datatypes}{}

\chapter{Datatypes}

I have tried to make RHIPE as flexible as possible with regards to data types exchanged. As such the following can be sent
\begin{itemize}
\item {} \begin{description}
\item[atomic vectors without attributes (raw, character, logical, complex, real, integer) (these include NA's)]\begin{itemize}
\item {} 
e.g names attribute will be lost

\end{itemize}

\end{description}

\item {} 
lists of the above and lists of lists. Names will be preserved.

\end{itemize}

So \emph{officially} no matrices, data.frames, time series objects etc. If there are needed serialized them using \code{serialize} .

\emph{Unofficially}, several attributes of are serialized, hence you can send matrices (the dim and  dimnames attribute are involved) will be serialized.
However, with regards to data.frames the row.names attribute has caused me much grief. Hence, relying on these types can lead to many problems, crashes even.

If in doubt, serialize.


\renewcommand{\indexname}{Module Index}
\printmodindex
\renewcommand{\indexname}{Index}
\printindex
\end{document}
